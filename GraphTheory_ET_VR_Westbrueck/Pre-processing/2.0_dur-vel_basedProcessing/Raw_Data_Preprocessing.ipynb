{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "050dead6",
   "metadata": {},
   "source": [
    "# Script: Raw_Data_Preprocessing\n",
    "- written by Jasmin L. Walter (jawalter@uos.de)\n",
    "- reads in nested json files and returns flattened csv files\n",
    "- does not change anything in the data, only extracts all variables from all 9 layers of the nested json files and saves them in data frames / csv files unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfa87cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General configuration\n",
    "import os\n",
    "\n",
    "# install_missing_packages: bool\n",
    "#     A flag indicating if missing packages should be automatically installed\n",
    "install_missing_packages = True\n",
    "\n",
    "# use_conda: bool\n",
    "#     A flag indicating if conda should be used for software installation.\n",
    "#     If False, pip will be used. The default is to use conda if jupyter\n",
    "#     is run in a conda environment.\n",
    "use_conda = 'CONDA_EXE' in os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baed198d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "def check_package(package, pip_pkg: str = None, conda_pkg: str = None):\n",
    "    \"\"\"Check if a given package is installed. If missing install\n",
    "    it (if global flag `install_missing_packages` is True) either with\n",
    "    pip or with conda (depending on `use_conda`).\n",
    "    \"\"\"\n",
    "    if importlib.util.find_spec(package) is not None:\n",
    "        return  # ok, package is already installed\n",
    "\n",
    "    if not install_missing_packages:\n",
    "        raise RuntimeError(f\"{package} is not installed!\")\n",
    "\n",
    "    if use_conda:\n",
    "        import conda.cli\n",
    "        conda.cli.main('conda', 'install',  '-y', conda_pkg or package)\n",
    "    else:\n",
    "        import subprocess\n",
    "        import sys            \n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', pip_pkg or package])\n",
    "        \n",
    "# This is to exit cells without error tracebacks (cosmetic purpose)\n",
    "class StopExecution(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f10e4b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "#import networkx as nx\n",
    "import glob\n",
    "import scipy.cluster.vq as clusters\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "#from sklearn.preprocessing import normalize\n",
    "from pandas.plotting import autocorrelation_plot as AC_plot \n",
    "from statsmodels.graphics import tsaplots\n",
    "from statsmodels.tsa.stattools import acf\n",
    "#from skimage.filters import gaussian\n",
    "from mpl_toolkits.mplot3d import Axes3D \n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ad0097",
   "metadata": {},
   "source": [
    "# Customize to run scripts - paths, subject ids to run etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2c98379",
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< Updated upstream
    "DATA_PATH = 'E:/WestbrookProject/Human_A_Data/Experiment1/Exploration_short/'\n",
    "\n",
    "PROCESSED_DATA_PATH = 'E:/WestbrookProject/Human_A_Data/Experiment1/pre-processing/'\n",
=======
    "DATA_PATH = 'D:/Jasmin/Human_A_Data/Experiment1/Exploration_short/raw_data/'\n",
    "\n",
    "PROCESSED_DATA_PATH = 'D:/Jasmin/Human_A_Data/Experiment1/Exploration_short/pre-processing/unflattening/'\n",
    "\n",
>>>>>>> Stashed changes
    "\n",
    "# Getting the Folder without hidden files in ascending order \n",
    "DATA_FOLDER = sorted([f for f in os.listdir(DATA_PATH) if not f.startswith('.')], key=str.lower)\n",
    "PROCESSED_DATA_FOLDER = sorted([f for f in os.listdir(PROCESSED_DATA_PATH) if not f.startswith('.')], key=str.lower)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53292cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 365  479 1754 2258 2361 2693 3246 3310 3572 3976 4176 4597 4796 4917\n",
      " 5238 5531 5741 6642 7093 7264 7412 7842 8007 8469 8673 8695 9472 9502\n",
<<<<<<< Updated upstream
      " 9586 9601]\n",
      "number data folders: 30\n"
=======
      " 9586 9601]\n"
>>>>>>> Stashed changes
     ]
    }
   ],
   "source": [
    "subIDs = []\n",
    "for sub in DATA_FOLDER:\n",
<<<<<<< Updated upstream
    "    if sub[0:4].isdigit():\n",
=======
    "#     if sub[0:4].isdigit() and sub.startswith('1'):\n",
    "    if sub[0:4].isdigit():\n",
    "\n",
>>>>>>> Stashed changes
    "        subIDs.append(int(sub[0:4]))\n",
    "    else:\n",
    "        pass\n",
    "subIDs = np.unique(subIDs)\n",
    "print(subIDs)\n",
    "print('number data folders:', len(subIDs))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 26,
=======
   "execution_count": 6,
>>>>>>> Stashed changes
   "id": "24aed9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< Updated upstream
      "22\n"
=======
      "['0365', '0479']\n",
      "number of participants 2\n"
>>>>>>> Stashed changes
     ]
    }
   ],
   "source": [
    "# # custom subIDs, if only a subset of participants should be processed\n",
<<<<<<< Updated upstream
    "subIDs = [1754, 2258, 2361, 2693, 3310, 4176, 4597, 4796, 4917,\n",
    " 5741, 6642, 7093, 7264, 7412, 7842, 8007, 8469, 8673, 9472, 9502,\n",
    " 9586, 9601]\n",
    "#   365,  479, \n",
    "print(len(subIDs))"
=======
    "\n",
    "# subIDs = [1027, 1029]\n",
    "# subIDs = [ 1754, 2258, 2693, 3310, 4176, 4597, 4796, 4917, 5741, 6642, 7093, 7412, 7842, 8007, 8469, 8673, 9472, 9502, 9586, 9601, 7264]\n",
    "subIDs=  [\"0365\",\"0479\"] # processs these separately, since anyoing 0 problem\n",
    "print(subIDs)\n",
    "print(\"number of participants\",len(subIDs))\n",
    "# process these 2 participants later 365,479"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4994d928",
   "metadata": {},
   "source": [
    "# Main part Human-A --> Tracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9543dae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 0365 started - 1/2 subjects\n",
      "15  files were found for participant  0365\n",
      "A maximum of  5 sessions were found and will be processed\n",
      "\tTotal Sessionfiles: 3 - Exploration Session 1\n",
      "load data of file  0365_Expl_S_01_ET_1_1635519946.47664.json\n",
      "Path:  D:/Jasmin/Human_A_Data/Experiment1/Exploration_short/raw_data/0365/0365_Expl_S_01_ET_1_1635519946.47664.json\n",
      "read file\n",
      "data loaded\n",
      "time is:  Wed Aug 14 22:08:30 2024\n",
      "trial info saved\n",
      "saving data\n",
      "data saved\n",
      "time is:  Wed Aug 14 22:36:00 2024\n",
      "load data of file  0365_Expl_S_01_ET_2_1635520742.30725.json\n",
      "Path:  D:/Jasmin/Human_A_Data/Experiment1/Exploration_short/raw_data/0365/0365_Expl_S_01_ET_2_1635520742.30725.json\n",
      "read file\n",
      "data loaded\n",
      "time is:  Wed Aug 14 22:36:10 2024\n",
      "trial info saved\n",
      "saving data\n",
      "data saved\n",
      "time is:  Wed Aug 14 23:03:20 2024\n",
      "load data of file  0365_Expl_S_01_ET_3_1635521652.17762.json\n",
      "Path:  D:/Jasmin/Human_A_Data/Experiment1/Exploration_short/raw_data/0365/0365_Expl_S_01_ET_3_1635521652.17762.json\n",
      "read file\n",
      "data loaded\n",
      "time is:  Wed Aug 14 23:03:30 2024\n",
      "trial info saved\n",
      "saving data\n",
      "data saved\n",
      "time is:  Wed Aug 14 23:33:12 2024\n",
      "\tTotal Sessionfiles: 3 - Exploration Session 2\n",
      "load data of file  0365_Expl_S_02_ET_1_1635764265.97509.json\n",
      "Path:  D:/Jasmin/Human_A_Data/Experiment1/Exploration_short/raw_data/0365/0365_Expl_S_02_ET_1_1635764265.97509.json\n",
      "read file\n",
      "data loaded\n",
      "time is:  Wed Aug 14 23:33:22 2024\n",
      "trial info saved\n",
      "saving data\n",
      "data saved\n",
      "time is:  Thu Aug 15 00:01:20 2024\n",
      "load data of file  0365_Expl_S_02_ET_2_1635764953.81019.json\n",
      "Path:  D:/Jasmin/Human_A_Data/Experiment1/Exploration_short/raw_data/0365/0365_Expl_S_02_ET_2_1635764953.81019.json\n",
      "read file\n",
      "data loaded\n",
      "time is:  Thu Aug 15 00:01:30 2024\n",
      "trial info saved\n",
      "saving data\n",
      "data saved\n",
      "time is:  Thu Aug 15 00:30:35 2024\n",
      "load data of file  0365_Expl_S_02_ET_3_1635765634.71963.json\n",
      "Path:  D:/Jasmin/Human_A_Data/Experiment1/Exploration_short/raw_data/0365/0365_Expl_S_02_ET_3_1635765634.71963.json\n",
      "read file\n",
      "data loaded\n",
      "time is:  Thu Aug 15 00:30:45 2024\n",
      "trial info saved\n",
      "saving data\n",
      "data saved\n",
      "time is:  Thu Aug 15 01:00:41 2024\n",
      "\tTotal Sessionfiles: 3 - Exploration Session 3\n",
      "load data of file  0365_Expl_S_03_ET_1_1636114599.55226.json\n",
      "Path:  D:/Jasmin/Human_A_Data/Experiment1/Exploration_short/raw_data/0365/0365_Expl_S_03_ET_1_1636114599.55226.json\n",
      "read file\n",
      "data loaded\n",
      "time is:  Thu Aug 15 01:00:50 2024\n",
      "trial info saved\n",
      "saving data\n",
      "data saved\n",
      "time is:  Thu Aug 15 01:29:32 2024\n",
      "load data of file  0365_Expl_S_03_ET_2_1636115267.44766.json\n",
      "Path:  D:/Jasmin/Human_A_Data/Experiment1/Exploration_short/raw_data/0365/0365_Expl_S_03_ET_2_1636115267.44766.json\n",
      "read file\n",
      "data loaded\n",
      "time is:  Thu Aug 15 01:29:42 2024\n",
      "trial info saved\n",
      "saving data\n",
      "data saved\n",
      "time is:  Thu Aug 15 02:00:29 2024\n",
      "load data of file  0365_Expl_S_03_ET_3_1636115934.61097.json\n",
      "Path:  D:/Jasmin/Human_A_Data/Experiment1/Exploration_short/raw_data/0365/0365_Expl_S_03_ET_3_1636115934.61097.json\n",
      "read file\n",
      "data loaded\n",
      "time is:  Thu Aug 15 02:00:38 2024\n",
      "trial info saved\n",
      "saving data\n",
      "data saved\n",
      "time is:  Thu Aug 15 02:31:06 2024\n",
      "\tTotal Sessionfiles: 3 - Exploration Session 4\n",
      "load data of file  0365_Expl_S_04_ET_1_1636622200.40475.json\n",
      "Path:  D:/Jasmin/Human_A_Data/Experiment1/Exploration_short/raw_data/0365/0365_Expl_S_04_ET_1_1636622200.40475.json\n",
      "read file\n",
      "data loaded\n",
      "time is:  Thu Aug 15 02:31:16 2024\n",
      "trial info saved\n",
      "saving data\n",
      "data saved\n",
      "time is:  Thu Aug 15 03:00:38 2024\n",
      "load data of file  0365_Expl_S_04_ET_2_1636622890.93561.json\n",
      "Path:  D:/Jasmin/Human_A_Data/Experiment1/Exploration_short/raw_data/0365/0365_Expl_S_04_ET_2_1636622890.93561.json\n",
      "read file\n",
      "data loaded\n",
      "time is:  Thu Aug 15 03:00:48 2024\n",
      "trial info saved\n",
      "saving data\n",
      "data saved\n",
      "time is:  Thu Aug 15 03:30:44 2024\n",
      "load data of file  0365_Expl_S_04_ET_3_1636623567.38353.json\n",
      "Path:  D:/Jasmin/Human_A_Data/Experiment1/Exploration_short/raw_data/0365/0365_Expl_S_04_ET_3_1636623567.38353.json\n",
      "read file\n",
      "data loaded\n",
      "time is:  Thu Aug 15 03:30:54 2024\n",
      "trial info saved\n",
      "saving data\n",
      "data saved\n",
      "time is:  Thu Aug 15 04:01:16 2024\n",
      "\tTotal Sessionfiles: 3 - Exploration Session 5\n",
      "load data of file  0365_Expl_S_05_ET_1_1636976820.6127.json\n",
      "Path:  D:/Jasmin/Human_A_Data/Experiment1/Exploration_short/raw_data/0365/0365_Expl_S_05_ET_1_1636976820.6127.json\n",
      "read file\n",
      "data loaded\n",
      "time is:  Thu Aug 15 04:01:25 2024\n",
      "trial info saved\n",
      "saving data\n",
      "data saved\n",
      "time is:  Thu Aug 15 04:26:50 2024\n",
      "load data of file  0365_Expl_S_05_ET_2_1636978159.00454.json\n",
      "Path:  D:/Jasmin/Human_A_Data/Experiment1/Exploration_short/raw_data/0365/0365_Expl_S_05_ET_2_1636978159.00454.json\n",
      "read file\n",
      "data loaded\n",
      "time is:  Thu Aug 15 04:26:59 2024\n",
      "trial info saved\n",
      "saving data\n",
      "data saved\n",
      "time is:  Thu Aug 15 04:58:09 2024\n",
      "load data of file  0365_Expl_S_05_ET_3_1636978829.12798.json\n",
      "Path:  D:/Jasmin/Human_A_Data/Experiment1/Exploration_short/raw_data/0365/0365_Expl_S_05_ET_3_1636978829.12798.json\n",
      "read file\n",
      "data loaded\n",
      "time is:  Thu Aug 15 04:58:18 2024\n",
      "trial info saved\n",
      "saving data\n",
      "data saved\n",
      "time is:  Thu Aug 15 05:29:27 2024\n",
      "Subject 0479 started - 2/2 subjects\n",
      "15  files were found for participant  0479\n",
      "A maximum of  5 sessions were found and will be processed\n",
      "\tTotal Sessionfiles: 3 - Exploration Session 1\n",
      "load data of file  0479_Expl_S_01_ET_1_1652967445.44017.json\n",
      "Path:  D:/Jasmin/Human_A_Data/Experiment1/Exploration_short/raw_data/0479/0479_Expl_S_01_ET_1_1652967445.44017.json\n",
      "read file\n",
      "data loaded\n",
      "time is:  Thu Aug 15 05:29:36 2024\n",
      "trial info saved\n",
      "saving data\n",
      "data saved\n",
      "time is:  Thu Aug 15 05:58:21 2024\n",
      "load data of file  0479_Expl_S_01_ET_2_1652968429.72041.json\n",
      "Path:  D:/Jasmin/Human_A_Data/Experiment1/Exploration_short/raw_data/0479/0479_Expl_S_01_ET_2_1652968429.72041.json\n",
      "read file\n",
      "data loaded\n",
      "time is:  Thu Aug 15 05:58:31 2024\n",
      "trial info saved\n",
      "saving data\n",
      "data saved\n",
      "time is:  Thu Aug 15 06:29:13 2024\n",
      "load data of file  0479_Expl_S_01_ET_3_1652969874.13837.json\n",
      "Path:  D:/Jasmin/Human_A_Data/Experiment1/Exploration_short/raw_data/0479/0479_Expl_S_01_ET_3_1652969874.13837.json\n",
      "read file\n",
      "data loaded\n",
      "time is:  Thu Aug 15 06:29:24 2024\n",
      "trial info saved\n",
      "saving data\n",
      "data saved\n",
      "time is:  Thu Aug 15 07:00:23 2024\n",
      "\tTotal Sessionfiles: 3 - Exploration Session 2\n",
      "load data of file  0479_Expl_S_02_ET_1_1653046022.19716.json\n",
      "Path:  D:/Jasmin/Human_A_Data/Experiment1/Exploration_short/raw_data/0479/0479_Expl_S_02_ET_1_1653046022.19716.json\n",
      "read file\n",
      "data loaded\n",
      "time is:  Thu Aug 15 07:00:33 2024\n",
      "trial info saved\n",
      "saving data\n",
      "data saved\n",
      "time is:  Thu Aug 15 07:30:12 2024\n",
      "load data of file  0479_Expl_S_02_ET_2_1653046869.72282.json\n",
      "Path:  D:/Jasmin/Human_A_Data/Experiment1/Exploration_short/raw_data/0479/0479_Expl_S_02_ET_2_1653046869.72282.json\n",
      "read file\n",
      "data loaded\n",
      "time is:  Thu Aug 15 07:30:22 2024\n",
      "trial info saved\n",
      "saving data\n",
      "data saved\n",
      "time is:  Thu Aug 15 08:00:20 2024\n",
      "load data of file  0479_Expl_S_02_ET_3_1653048047.695.json\n",
      "Path:  D:/Jasmin/Human_A_Data/Experiment1/Exploration_short/raw_data/0479/0479_Expl_S_02_ET_3_1653048047.695.json\n",
      "read file\n",
      "data loaded\n",
      "time is:  Thu Aug 15 08:00:31 2024\n",
      "trial info saved\n",
      "saving data\n",
      "data saved\n",
      "time is:  Thu Aug 15 08:32:19 2024\n",
      "\tTotal Sessionfiles: 3 - Exploration Session 3\n",
      "load data of file  0479_Expl_S_03_ET_1_1653309121.0652.json\n",
      "Path:  D:/Jasmin/Human_A_Data/Experiment1/Exploration_short/raw_data/0479/0479_Expl_S_03_ET_1_1653309121.0652.json\n",
      "read file\n",
      "data loaded\n",
      "time is:  Thu Aug 15 08:32:30 2024\n",
      "trial info saved\n",
      "saving data\n",
      "data saved\n",
      "time is:  Thu Aug 15 09:03:35 2024\n",
      "load data of file  0479_Expl_S_03_ET_2_1653309917.04279.json\n",
      "Path:  D:/Jasmin/Human_A_Data/Experiment1/Exploration_short/raw_data/0479/0479_Expl_S_03_ET_2_1653309917.04279.json\n",
      "read file\n",
      "data loaded\n",
      "time is:  Thu Aug 15 09:03:44 2024\n",
      "trial info saved\n",
      "saving data\n",
      "data saved\n",
      "time is:  Thu Aug 15 09:34:30 2024\n",
      "load data of file  0479_Expl_S_03_ET_3_1653310821.18151.json\n",
      "Path:  D:/Jasmin/Human_A_Data/Experiment1/Exploration_short/raw_data/0479/0479_Expl_S_03_ET_3_1653310821.18151.json\n",
      "read file\n",
      "data loaded\n",
      "time is:  Thu Aug 15 09:34:40 2024\n",
      "trial info saved\n",
      "saving data\n",
      "data saved\n",
      "time is:  Thu Aug 15 10:05:26 2024\n",
      "\tTotal Sessionfiles: 3 - Exploration Session 4\n",
      "load data of file  0479_Expl_S_04_ET_1_1653398546.80713.json\n",
      "Path:  D:/Jasmin/Human_A_Data/Experiment1/Exploration_short/raw_data/0479/0479_Expl_S_04_ET_1_1653398546.80713.json\n",
      "read file\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Unterminated string starting at: line 1 column 154247168 (char 154247167)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-e5d8da72969d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    120\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"reading did not work\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m                 \u001b[0msubject_session\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data loaded\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'time is: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\graphs\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mloads\u001b[1;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    352\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[1;32m--> 354\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    355\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\graphs\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m         \"\"\"\n\u001b[1;32m--> 339\u001b[1;33m         \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    340\u001b[0m         \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\graphs\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    353\u001b[0m         \"\"\"\n\u001b[0;32m    354\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m             \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Expecting value\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Unterminated string starting at: line 1 column 154247168 (char 154247167)"
     ]
    }
   ],
   "source": [
    "# if no ray cast information is available, the data frame will be filled with nan values\n",
    "# create empty data frames with nan values and correct variable names\n",
    "#\n",
    "\n",
    "columns1 = ['hitObjectColliderName_1','ordinalOfHit_1', 'hitColliderType_1',\n",
    "            'hitPointOnObject.x_1','hitPointOnObject.y_1','hitPointOnObject.z_1',\n",
    "            'hitObjectColliderBoundsCenter.x_1','hitObjectColliderBoundsCenter.y_1','hitObjectColliderBoundsCenter.z_1']\n",
    "\n",
    "columns2 = ['hitObjectColliderName_2','ordinalOfHit_2', 'hitColliderType_2',\n",
    "            'hitPointOnObject.x_2','hitPointOnObject.y_2','hitPointOnObject.z_2',\n",
    "            'hitObjectColliderBoundsCenter.x_2','hitObjectColliderBoundsCenter.y_2','hitObjectColliderBoundsCenter.z_2']\n",
    "\n",
    "columnsRCall = ['hitObjectColliderName_1','ordinalOfHit_1','hitColliderType_1',\n",
    "                'hitPointOnObject.x_1','hitPointOnObject.y_1','hitPointOnObject.z_1',\n",
    "                'hitObjectColliderBoundsCenter.x_1','hitObjectColliderBoundsCenter.y_1','hitObjectColliderBoundsCenter.z_1',\n",
    "                'hitObjectColliderName_2','ordinalOfHit_2','hitColliderType_2',\n",
    "                'hitPointOnObject.x_2','hitPointOnObject.y_2','hitPointOnObject.z_2',\n",
    "                'hitObjectColliderBoundsCenter.x_2','hitObjectColliderBoundsCenter.y_2','hitObjectColliderBoundsCenter.z_2',\n",
    "                'DataRow']\n",
    "\n",
    "emptyDF1 = pd.DataFrame(np.nan,index=[0], columns= columns1)\n",
    "emptyDF2 = pd.DataFrame(np.nan,index=[0], columns= columns2)\n",
    "\n",
    "\n",
    "\n",
    "#########################################################################################################\n",
    "# data loop through all subjects and sessions\n",
    "\n",
    "subcount = 0\n",
    "\n",
    "\n",
    "for subject in subIDs:\n",
    "    \n",
    "    subcount +=1\n",
    "    print('Subject ' \n",
    "          + str(subject) \n",
    "          + ' started - ' \n",
    "          + str(subcount) \n",
    "          + '/' \n",
    "          + str(len(subIDs)) \n",
    "          + ' subjects')\n",
    "    \n",
    "#     # Create empty dataframe for later concatenation\n",
    "# complete_exploration_df = pd.DataFrame(columns = col_names)\n",
    "#     complete_exploration_df.head()\n",
    "    \n",
    "    \n",
    "    # change dir into the subject folder \n",
    "    CURRENT_SUBJECT_FOLDER = sorted([f for f in os.listdir(DATA_PATH + subject) if not f.startswith('.')], key=str.lower)\n",
    "    # get the data files according to the subject, ignoring OnQuit files\n",
    "    subject_files = sorted([f for f in CURRENT_SUBJECT_FOLDER \n",
    "                             if f.startswith(subject + '_Expl_S_') and not f.endswith(\"OnQuit.json\")], \n",
    "                            key=str.lower) \n",
    "    \n",
    "    # the following works as long as the data name format is as follows:\n",
    "    # 'subjectID'_Expl_S_'SessionNumber'_ET_'EyeTrackingSessionNumber'_'UnixTimestamp'.json\n",
    "    folder_files = list()\n",
    "    \n",
    "    # loop through the subject folder and save all numbers\n",
    "    for file in subject_files:\n",
    "        folder_files.append(re.findall(r'\\d+', file))\n",
    "    \n",
    "    # Extract all SubIDs (only one), SessionNumbers, ET_SessionNumbers (and Timestamps)\n",
    "    try:\n",
    "        SubID, SessionNumbers, ET_SessionNumbers, UnixTimestamp1, UnixTimeStamp2 = map(list, zip(*folder_files))\n",
    "    except:\n",
    "        print('\\tSubject ' \n",
    "              + subject\n",
    "              + ' Filename is not valid!')\n",
    "        \n",
    "#     print(SubID)\n",
    "#     print(SessionNumbers)\n",
    "#     print(ET_SessionNumbers)\n",
    "#     print(UnixTimestamp1)\n",
    "#     print(UnixTimeStamp2)\n",
    "    \n",
    "    session_number = int(max(SessionNumbers)) # the maximum session number of the particular subject\n",
    "    ET_session_number = int(max(ET_SessionNumbers)) # the maximum ET session number of the particular subject\n",
    "    \n",
    "    \n",
    "    # print info of how many files were found \n",
    "    \n",
    "    print(len(SubID), ' files were found for participant ', SubID[0])\n",
    "    print('A maximum of ', session_number, 'sessions were found and will be processed')\n",
    "        \n",
    "# --------- second layer - exploration session loop ---------\n",
    "\n",
    "    # loop over exploration sessions\n",
    "    for EXP_session in range(session_number):\n",
    "        # to avoid start at 0\n",
    "        EXP_session +=1 \n",
    "\n",
    "        # extract the exploration data files for each session - but exclude OnQuit files\n",
    "        subject_data = sorted([f for f in CURRENT_SUBJECT_FOLDER if f.startswith(subject + '_Expl_S_0' + str(EXP_session)) \n",
    "                               and not f.endswith(\"OnQuit.json\")], key=str.lower)\n",
    "\n",
    "    \n",
    "\n",
    "        print(f\"\\tTotal Sessionfiles: {len(subject_data)} - Exploration Session {EXP_session}\")\n",
    "\n",
    "\n",
    "        ET_session_count = 0 # session count\n",
    "\n",
    "# --------- third layer - eye tracking session loop ---------\n",
    "\n",
    "        # loop over separate eye tracking sessions\n",
    "        for fileName in subject_data:\n",
    "            ET_session_count+=1\n",
    "\n",
    "            print('load data of file ', fileName)\n",
    "\n",
    "            print('Path: ', DATA_PATH + str(subject) + '/' + fileName)\n",
    "            # open the JSON file as dictionary\n",
    "            with open(DATA_PATH + str(subject) + '/' + fileName) as datafile:\n",
    "                try:\n",
    "                    print(\"read file\")\n",
    "                    dataR = '['+ datafile.read()\n",
    "                    dataR = dataR[:len(dataR)] + \"]\"\n",
    "                except:\n",
    "                    print(\"reading did not work\")\n",
    "\n",
    "                subject_session = json.loads(dataR)\n",
    "                print(\"data loaded\")\n",
    "                print('time is: ', time.ctime())\n",
    "\n",
    "\n",
    "\n",
    "##################################################################################################################\n",
    "\n",
    "            # Data flattening part: \n",
    "            # first save the overall trial information\n",
    "\n",
    "\n",
    "            infoDF = pd.json_normalize(subject_session[0]['trials'][0])\n",
    "            infoDF = infoDF.drop(columns=['dataPoints'])\n",
    "            infoDF.insert(0,'FileInfo',fileName[0:18])\n",
    "            infoDF.to_csv(PROCESSED_DATA_PATH + fileName[0:12] + fileName[13:19] + '_infoSummary.csv', index = False)\n",
    "            print('trial info saved')\n",
    "            \n",
    "\n",
    "            # flatten the majority of the variables into currentDF data frame\n",
    "            currentDF_raw = pd.json_normalize(subject_session[0]['trials'][0]['dataPoints'])\n",
    "\n",
    "            # remove the 'rayCastHitsCombinedEyes' column as it still contains a nested data structure\n",
    "            dataDF = currentDF_raw.drop(columns=['rayCastHitsCombinedEyes'])\n",
    "            \n",
    "            # create an empty data frame of the required size\n",
    "            rayCastData_df = pd.DataFrame(np.nan,index=range(len(subject_session[0]['trials'][0]['dataPoints'])), columns= columnsRCall)\n",
    "\n",
    "            # now loop through the individual trials and flatten the data\n",
    "            for index in range(len(subject_session[0]['trials'][0]['dataPoints'])):\n",
    "                \n",
    "                # depending on the size of the ray cast data - flatten data and appand it to currentDF data frame\n",
    "                # the variables are renamed to make the differentiation of first and second order collider hits more intuitive\n",
    "                #lengthRCData = len(subject_session[0]['trials'][0]['dataPoints'][index]['rayCastHitsCombinedEyes'][0])\n",
    "                lengthRCData = len(currentDF_raw.at[index,'rayCastHitsCombinedEyes'])\n",
    "                \n",
    "                \n",
    "                if lengthRCData ==0: #case: no ray cast data is available = no collider was hit\n",
    "\n",
    "                    combineDF = pd.concat([emptyDF1, emptyDF2], axis=1)\n",
    "                    combineDF.insert(len(combineDF.columns), 'DataRow',index)\n",
    "\n",
    "\n",
    "                elif lengthRCData == 1: # case: only one collider was hit, there is no secondary hit\n",
    "\n",
    "                    pdRC1= pd.json_normalize(currentDF_raw.at[index,'rayCastHitsCombinedEyes'][0]).rename(\n",
    "                        columns = {'hitObjectColliderName':'hitObjectColliderName_1',\n",
    "                                   'ordinalOfHit':'ordinalOfHit_1',\n",
    "                                   'hitColliderType' : 'hitColliderType_1',\n",
    "                                   'hitPointOnObject.x':'hitPointOnObject.x_1',\n",
    "                                   'hitPointOnObject.y':'hitPointOnObject.y_1',\n",
    "                                   'hitPointOnObject.z':'hitPointOnObject.z_1',\n",
    "                                   'hitObjectColliderBoundsCenter.x':'hitObjectColliderBoundsCenter.x_1',\n",
    "                                   'hitObjectColliderBoundsCenter.y':'hitObjectColliderBoundsCenter.y_1',\n",
    "                                   'hitObjectColliderBoundsCenter.z':'hitObjectColliderBoundsCenter.z_1'})\n",
    "                    combineDF = pd.concat([pdRC1, emptyDF2], axis=1)\n",
    "                    combineDF.insert(len(combineDF.columns), 'DataRow',index)\n",
    "\n",
    "                elif lengthRCData == 2: # case: two collider were hit \n",
    "\n",
    "                    pdRC1= pd.json_normalize(currentDF_raw.at[index,'rayCastHitsCombinedEyes'][0]).rename(\n",
    "                        columns = {'hitObjectColliderName':'hitObjectColliderName_1',\n",
    "                                   'ordinalOfHit':'ordinalOfHit_1',\n",
    "                                   'hitColliderType' : 'hitColliderType_1',\n",
    "                                   'hitPointOnObject.x':'hitPointOnObject.x_1',\n",
    "                                   'hitPointOnObject.y':'hitPointOnObject.y_1',\n",
    "                                   'hitPointOnObject.z':'hitPointOnObject.z_1',\n",
    "                                   'hitObjectColliderBoundsCenter.x':'hitObjectColliderBoundsCenter.x_1',\n",
    "                                   'hitObjectColliderBoundsCenter.y':'hitObjectColliderBoundsCenter.y_1',\n",
    "                                   'hitObjectColliderBoundsCenter.z':'hitObjectColliderBoundsCenter.z_1'})\n",
    "\n",
    "                    pdRC2 = pd.json_normalize(currentDF_raw.at[index,'rayCastHitsCombinedEyes'][1]).rename(\n",
    "                        columns = {'hitObjectColliderName':'hitObjectColliderName_2',\n",
    "                                   'ordinalOfHit':'ordinalOfHit_2',\n",
    "                                   'hitColliderType' : 'hitColliderType_2',\n",
    "                                   'hitPointOnObject.x':'hitPointOnObject.x_2',\n",
    "                                   'hitPointOnObject.y':'hitPointOnObject.y_2',\n",
    "                                   'hitPointOnObject.z':'hitPointOnObject.z_2',\n",
    "                                   'hitObjectColliderBoundsCenter.x':'hitObjectColliderBoundsCenter.x_2',\n",
    "                                   'hitObjectColliderBoundsCenter.y':'hitObjectColliderBoundsCenter.y_2',\n",
    "                                   'hitObjectColliderBoundsCenter.z':'hitObjectColliderBoundsCenter.z_2'})\n",
    "                    combineDF = pd.concat([pdRC1, pdRC2], axis=1)\n",
    "                    combineDF.insert(len(combineDF.columns), 'DataRow',index)\n",
    "\n",
    "\n",
    "                else:\n",
    "                    print('!!!an exception occured in the ray cast data flattening in trial ', index)\n",
    "\n",
    "                # now add the new data row to the data overview\n",
    "                # rayCastData_df = [rayCastData_df]\n",
    "\n",
    "            \n",
    "                rayCastData_df.loc[index] = combineDF.loc[0]\n",
    "                \n",
    "            flatData_df = pd.concat([dataDF,rayCastData_df],axis=1)   \n",
    "\n",
    "            print('saving data')\n",
    "            flatData_df.to_csv(PROCESSED_DATA_PATH + fileName[0:12] + fileName[13:19] + '_flattened.csv', index = False)\n",
    "            print('data saved')\n",
    "            print('time is: ', time.ctime())\n"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0788e108",
   "metadata": {},
   "source": [
    "# Main part - flatten all nested data structures and save as csv - SpaReData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "89e8ac41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 1754 started - 1/22 subjects\n",
      "15  files were found for participant  1754\n",
      "A maximum of  5 sessions were found and will be processed\n",
<<<<<<< Updated upstream
      "A total of  15 files were found and will be processed\n",
      "\tTotal Sessionfiles: 3 - Exploration Session 1\n",
      "load data of file  1754_Expl_S_01_ET_1_1646740718.03586.json\n",
      "Path:  E:/WestbrookProject/Human_A_Data/Experiment1/Exploration_short/1754/1754_Expl_S_01_ET_1_1646740718.03586.json\n",
      "read file\n",
      "data loaded\n",
      "time is:  Wed Jul 31 20:04:33 2024\n",
      "trial info saved\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-18dba35caa48>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m             \u001b[1;31m# flatten the majority of the variables into currentDF data frame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m             \u001b[0mcurrentDF_raw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson_normalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubject_session\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'trials'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dataPoints'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m             \u001b[1;31m# remove the 'rayCastHitsCombinedEyes' column as it still contains a nested data structure\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Software\\Programme\\Anaconda3\\envs\\graphs\\lib\\site-packages\\pandas\\io\\json\\_normalize.py\u001b[0m in \u001b[0;36m_json_normalize\u001b[1;34m(data, record_path, meta, meta_prefix, record_prefix, errors, sep, max_level)\u001b[0m\n\u001b[0;32m    276\u001b[0m             \u001b[1;31m# TODO: handle record value which are lists, at least error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m             \u001b[1;31m#       reasonably\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 278\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnested_to_record\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_level\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_level\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    279\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecord_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Software\\Programme\\Anaconda3\\envs\\graphs\\lib\\site-packages\\pandas\\io\\json\\_normalize.py\u001b[0m in \u001b[0;36mnested_to_record\u001b[1;34m(ds, prefix, sep, level, max_level)\u001b[0m\n\u001b[0;32m     93\u001b[0m             \u001b[1;31m# only dicts gets recurse-flattened\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m             \u001b[1;31m# only at level>1 do we rename the rest of the keys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m             if not isinstance(v, dict) or (\n\u001b[0m\u001b[0;32m     96\u001b[0m                 \u001b[0mmax_level\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlevel\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mmax_level\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m             ):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
=======
      "\tTotal Sessionfiles: 0 - Exploration Session 1\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 2\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 3\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 4\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 5\n",
      "Subject 2258 started - 2/22 subjects\n",
      "15  files were found for participant  2258\n",
      "A maximum of  5 sessions were found and will be processed\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 1\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 2\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 3\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 4\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 5\n",
      "Subject 2693 started - 3/22 subjects\n",
      "15  files were found for participant  2693\n",
      "A maximum of  5 sessions were found and will be processed\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 1\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 2\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 3\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 4\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 5\n",
      "Subject 3310 started - 4/22 subjects\n",
      "16  files were found for participant  3310\n",
      "A maximum of  5 sessions were found and will be processed\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 1\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 2\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 3\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 4\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 5\n",
      "Subject 4176 started - 5/22 subjects\n",
      "15  files were found for participant  4176\n",
      "A maximum of  5 sessions were found and will be processed\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 1\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 2\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 3\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 4\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 5\n",
      "Subject 4597 started - 6/22 subjects\n",
      "15  files were found for participant  4597\n",
      "A maximum of  5 sessions were found and will be processed\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 1\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 2\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 3\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 4\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 5\n",
      "Subject 4796 started - 7/22 subjects\n",
      "15  files were found for participant  4796\n",
      "A maximum of  5 sessions were found and will be processed\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 1\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 2\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 3\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 4\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 5\n",
      "Subject 4917 started - 8/22 subjects\n",
      "15  files were found for participant  4917\n",
      "A maximum of  5 sessions were found and will be processed\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 1\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 2\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 3\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 4\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 5\n",
      "Subject 5741 started - 9/22 subjects\n",
      "17  files were found for participant  5741\n",
      "A maximum of  5 sessions were found and will be processed\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 1\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 2\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 3\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 4\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 5\n",
      "Subject 6642 started - 10/22 subjects\n",
      "15  files were found for participant  6642\n",
      "A maximum of  5 sessions were found and will be processed\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 1\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 2\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 3\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 4\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 5\n",
      "Subject 7093 started - 11/22 subjects\n",
      "15  files were found for participant  7093\n",
      "A maximum of  5 sessions were found and will be processed\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 1\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 2\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 3\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 4\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 5\n",
      "Subject 7412 started - 12/22 subjects\n",
      "15  files were found for participant  7412\n",
      "A maximum of  5 sessions were found and will be processed\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 1\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 2\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 3\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 4\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 5\n",
      "Subject 7842 started - 13/22 subjects\n",
      "15  files were found for participant  7842\n",
      "A maximum of  5 sessions were found and will be processed\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 1\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 2\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 3\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 4\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 5\n",
      "Subject 8007 started - 14/22 subjects\n",
      "15  files were found for participant  8007\n",
      "A maximum of  5 sessions were found and will be processed\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 1\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 2\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 3\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 4\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 5\n",
      "Subject 8469 started - 15/22 subjects\n",
      "16  files were found for participant  8469\n",
      "A maximum of  5 sessions were found and will be processed\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 1\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 2\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 3\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 4\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 5\n",
      "Subject 8673 started - 16/22 subjects\n",
      "15  files were found for participant  8673\n",
      "A maximum of  5 sessions were found and will be processed\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 1\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 2\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 3\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 4\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 5\n",
      "Subject 9472 started - 17/22 subjects\n",
      "15  files were found for participant  9472\n",
      "A maximum of  5 sessions were found and will be processed\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 1\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 2\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 3\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 4\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 5\n",
      "Subject 9502 started - 18/22 subjects\n",
      "15  files were found for participant  9502\n",
      "A maximum of  5 sessions were found and will be processed\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 1\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 2\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 3\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 4\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 5\n",
      "Subject 9586 started - 19/22 subjects\n",
      "15  files were found for participant  9586\n",
      "A maximum of  5 sessions were found and will be processed\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 1\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 2\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 3\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 4\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 5\n",
      "Subject 9601 started - 20/22 subjects\n",
      "15  files were found for participant  9601\n",
      "A maximum of  5 sessions were found and will be processed\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 1\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 2\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 3\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 4\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 5\n",
      "Subject 7264 started - 21/22 subjects\n",
      "16  files were found for participant  7264\n",
      "A maximum of  5 sessions were found and will be processed\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 1\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 2\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 3\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 4\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 5\n",
      "Subject 2361 started - 22/22 subjects\n",
      "15  files were found for participant  2361\n",
      "A maximum of  5 sessions were found and will be processed\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 1\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 2\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 3\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 4\n",
      "\tTotal Sessionfiles: 0 - Exploration Session 5\n"
>>>>>>> Stashed changes
     ]
    }
   ],
   "source": [
    "# if no ray cast information is available, the data frame will be filled with nan values\n",
    "# create empty data frames with nan values and correct variable names\n",
<<<<<<< Updated upstream
    "columns1 = ['hitObjectColliderName_1','ordinalOfHit_1', 'hitColliderType_1','hitPointOnObject.x_1','hitPointOnObject.y_1','hitPointOnObject.z_1',\n",
=======
    "#\n",
    "\n",
    "columns1 = ['hitObjectColliderName_1','ordinalOfHit_1','hitPointOnObject.x_1','hitPointOnObject.y_1','hitPointOnObject.z_1',\n",
>>>>>>> Stashed changes
    "            'hitObjectColliderBoundsCenter.x_1','hitObjectColliderBoundsCenter.y_1','hitObjectColliderBoundsCenter.z_1']\n",
    "\n",
    "columns2 = ['hitObjectColliderName_2','ordinalOfHit_2','hitColliderType_2','hitPointOnObject.x_2','hitPointOnObject.y_2','hitPointOnObject.z_2',\n",
    "            'hitObjectColliderBoundsCenter.x_2','hitObjectColliderBoundsCenter.y_2','hitObjectColliderBoundsCenter.z_2']\n",
    "\n",
    "columnsRCall = ['hitObjectColliderName_1','ordinalOfHit_1','hitColliderType_1',\n",
    "                'hitPointOnObject.x_1','hitPointOnObject.y_1','hitPointOnObject.z_1',\n",
    "                'hitObjectColliderBoundsCenter.x_1','hitObjectColliderBoundsCenter.y_1','hitObjectColliderBoundsCenter.z_1',\n",
    "                'hitObjectColliderName_2','ordinalOfHit_2','hitColliderType_2',\n",
    "                'hitPointOnObject.x_2','hitPointOnObject.y_2','hitPointOnObject.z_2',\n",
    "                'hitObjectColliderBoundsCenter.x_2','hitObjectColliderBoundsCenter.y_2','hitObjectColliderBoundsCenter.z_2',\n",
    "                'DataRow']\n",
    "\n",
    "emptyDF1 = pd.DataFrame(np.nan,index=[0], columns= columns1)\n",
    "emptyDF2 = pd.DataFrame(np.nan,index=[0], columns= columns2)\n",
    "\n",
    "\n",
    "\n",
    "#########################################################################################################\n",
    "# data loop through all subjects and sessions\n",
    "\n",
    "subcount = 0\n",
    "\n",
    "\n",
    "for subject in subIDs:\n",
    "    \n",
    "    subcount +=1\n",
    "    print('Subject ' \n",
    "          + str(subject) \n",
    "          + ' started - ' \n",
    "          + str(subcount) \n",
    "          + '/' \n",
    "          + str(len(subIDs)) \n",
    "          + ' subjects')\n",
    "    \n",
    "#     # Create empty dataframe for later concatenation\n",
    "# complete_exploration_df = pd.DataFrame(columns = col_names)\n",
    "#     complete_exploration_df.head()\n",
    "    \n",
    "    \n",
    "    # change dir into the subject folder \n",
    "    CURRENT_SUBJECT_FOLDER = sorted([f for f in os.listdir(DATA_PATH+str(subject)) if not f.startswith('.')], key=str.lower)\n",
    "    # get the data files according to the subject, ignoring OnQuit files\n",
    "    subject_files = sorted([f for f in CURRENT_SUBJECT_FOLDER \n",
    "                             if f.startswith(str(subject)+'_Expl_S_') and f.endswith(\"OnQuit.json\") == False], \n",
    "                            key=str.lower) \n",
    "    \n",
    "    # the following works as long as the data name format is as follows:\n",
    "    # 'subjectID'_Expl_S_'SessionNumber'_ET_'EyeTrackingSessionNumber'_'UnixTimestamp'.json\n",
    "    folder_files = list()\n",
    "    \n",
    "    # loop through the subject folder and save all numbers\n",
    "    for file in subject_files:\n",
    "        folder_files.append(re.findall(r'\\d+', file))\n",
    "    \n",
    "    # Extract all SubIDs (only one), SessionNumbers, ET_SessionNumbers (and Timestamps)\n",
    "    try:\n",
    "        SubID, SessionNumbers, ET_SessionNumbers, UnixTimestamp1, UnixTimeStamp2 = map(list, zip(*folder_files))\n",
    "\n",
    "    except:\n",
    "        print('\\tSubject ' \n",
    "              + str(subject)\n",
    "              + ' Filename is not valid!')\n",
    "        \n",
    "#     print(SubID)\n",
    "#     print(SessionNumbers)\n",
    "#     print(ET_SessionNumbers)\n",
    "#     print(UnixTimestamp1)\n",
    "#     print(UnixTimeStamp2)\n",
    "    \n",
    "    session_number = int(max(SessionNumbers)) # the maximum session number of the particular subject\n",
    "    ET_session_number = int(max(ET_SessionNumbers)) # the maximum ET session number of the particular subject\n",
    "    \n",
    "    \n",
    "    # print info of how many files were found \n",
    "    \n",
    "    print(len(SubID), ' files were found for participant ', SubID[0])\n",
    "    print('A maximum of ', session_number, 'sessions were found and will be processed')\n",
    "    print('A total of ', session_number*ET_session_number, 'files were found and will be processed')\n",
    "\n",
    "        \n",
    "# --------- second layer - exploration session loop ---------\n",
    "\n",
    "    # loop over exploration sessions\n",
    "    for EXP_session in range(session_number):\n",
    "        # to avoid start at 0\n",
    "        EXP_session +=1 \n",
    "\n",
    "        # extract the exploration data files for each session - but exclude OnQuit files\n",
    "        subject_data = sorted([f for f in CURRENT_SUBJECT_FOLDER if f.startswith(str(subject) + '_Expl_S_0' + str(EXP_session)) \n",
    "                               and f.endswith(\"OnQuit.json\") == False], key=str.lower)\n",
    "\n",
    "    \n",
    "\n",
    "        print(\"\\tTotal Sessionfiles: \"\n",
    "              + str(len(subject_data))\n",
    "              + \" - Exploration Session \"\n",
    "              + str(EXP_session))\n",
    "\n",
    "        ET_session_count = 0 # session count\n",
    "\n",
    "# --------- third layer - eye tracking session loop ---------\n",
    "\n",
    "        # loop over separate eye tracking sessions\n",
    "        for fileName in subject_data:\n",
    "            ET_session_count+=1\n",
    "\n",
    "            print('load data of file ', fileName)\n",
    "\n",
    "            print('Path: ', DATA_PATH + str(subject) + '/' + fileName)\n",
    "            # open the JSON file as dictionary\n",
    "            with open(DATA_PATH + str(subject) + '/' + fileName) as datafile:\n",
    "                try:\n",
    "                    print(\"read file\")\n",
    "                    dataR = '['+ datafile.read()\n",
    "                    dataR = dataR[:len(dataR)] + \"]\"\n",
    "                except:\n",
    "                    print(\"reading did not work\")\n",
    "\n",
    "                subject_session = json.loads(dataR)\n",
    "                print(\"data loaded\")\n",
    "                print('time is: ', time.ctime())\n",
    "\n",
    "\n",
    "\n",
    "##################################################################################################################\n",
    "\n",
    "            # Data flattening part: \n",
    "            # first save the overall trial information\n",
    "\n",
    "\n",
    "            infoDF = pd.json_normalize(subject_session[0]['trials'][0])\n",
    "            infoDF = infoDF.drop(columns=['dataPoints'])\n",
    "            infoDF.insert(0,'FileInfo',fileName[0:18])\n",
    "#             infoDF.to_csv(PROCESSED_DATA_PATH + fileName[0:18] + '_infoSummary.csv', index = False)\n",
    "            infoDF.to_csv(PROCESSED_DATA_PATH + fileName[0:12] + fileName[13:19] + '_infoSummary.csv', index = False)\n",
    "\n",
    "            \n",
    "            print('trial info saved')\n",
    "            \n",
    "\n",
    "            # flatten the majority of the variables into currentDF data frame\n",
    "            currentDF_raw = pd.json_normalize(subject_session[0]['trials'][0]['dataPoints'])\n",
    "\n",
    "            # remove the 'rayCastHitsCombinedEyes' column as it still contains a nested data structure\n",
    "            dataDF = currentDF_raw.drop(columns=['rayCastHitsCombinedEyes'])\n",
    "            \n",
    "            # create an empty data frame of the required size\n",
    "            rayCastData_df = pd.DataFrame(np.nan,index=range(len(subject_session[0]['trials'][0]['dataPoints'])), columns= columnsRCall)\n",
    "\n",
    "            # now loop through the individual trials and flatten the data\n",
    "            for index in range(len(subject_session[0]['trials'][0]['dataPoints'])):\n",
    "                \n",
    "                # depending on the size of the ray cast data - flatten data and appand it to currentDF data frame\n",
    "                # the variables are renamed to make the differentiation of first and second order collider hits more intuitive\n",
    "                #lengthRCData = len(subject_session[0]['trials'][0]['dataPoints'][index]['rayCastHitsCombinedEyes'][0])\n",
    "                lengthRCData = len(currentDF_raw.at[index,'rayCastHitsCombinedEyes'])\n",
    "                \n",
    "                \n",
    "                if lengthRCData ==0: #case: no ray cast data is available = no collider was hit\n",
    "\n",
    "                    combineDF = pd.concat([emptyDF1, emptyDF2], axis=1)\n",
    "                    combineDF.insert(len(combineDF.columns), 'DataRow',index)\n",
    "\n",
    "\n",
    "                elif lengthRCData == 1: # case: only one collider was hit, there is no secondary hit\n",
    "\n",
    "                    pdRC1= pd.json_normalize(currentDF_raw.at[index,'rayCastHitsCombinedEyes'][0]).rename(\n",
    "                        columns = {'hitObjectColliderName':'hitObjectColliderName_1',\n",
    "                                   'ordinalOfHit':'ordinalOfHit_1',\n",
    "                                   'hitColliderType':'hitColliderType_1',\n",
    "                                   'hitPointOnObject.x':'hitPointOnObject.x_1',\n",
    "                                   'hitPointOnObject.y':'hitPointOnObject.y_1',\n",
    "                                   'hitPointOnObject.z':'hitPointOnObject.z_1',\n",
    "                                   'hitObjectColliderBoundsCenter.x':'hitObjectColliderBoundsCenter.x_1',\n",
    "                                   'hitObjectColliderBoundsCenter.y':'hitObjectColliderBoundsCenter.y_1',\n",
    "                                   'hitObjectColliderBoundsCenter.z':'hitObjectColliderBoundsCenter.z_1'})\n",
    "                    combineDF = pd.concat([pdRC1, emptyDF2], axis=1)\n",
    "                    combineDF.insert(len(combineDF.columns), 'DataRow',index)\n",
    "\n",
    "                elif lengthRCData == 2: # case: two collider were hit \n",
    "\n",
    "                    pdRC1= pd.json_normalize(currentDF_raw.at[index,'rayCastHitsCombinedEyes'][0]).rename(\n",
    "                        columns = {'hitObjectColliderName':'hitObjectColliderName_1',\n",
    "                                   'ordinalOfHit':'ordinalOfHit_1',\n",
    "                                   'hitColliderType':'hitColliderType_1',\n",
    "                                   'hitPointOnObject.x':'hitPointOnObject.x_1',\n",
    "                                   'hitPointOnObject.y':'hitPointOnObject.y_1',\n",
    "                                   'hitPointOnObject.z':'hitPointOnObject.z_1',\n",
    "                                   'hitObjectColliderBoundsCenter.x':'hitObjectColliderBoundsCenter.x_1',\n",
    "                                   'hitObjectColliderBoundsCenter.y':'hitObjectColliderBoundsCenter.y_1',\n",
    "                                   'hitObjectColliderBoundsCenter.z':'hitObjectColliderBoundsCenter.z_1'})\n",
    "\n",
    "                    pdRC2 = pd.json_normalize(currentDF_raw.at[index,'rayCastHitsCombinedEyes'][1]).rename(\n",
    "                        columns = {'hitObjectColliderName':'hitObjectColliderName_2',\n",
    "                                   'ordinalOfHit':'ordinalOfHit_2',\n",
    "                                   'hitColliderType':'hitColliderType_2',\n",
    "                                   'hitPointOnObject.x':'hitPointOnObject.x_2',\n",
    "                                   'hitPointOnObject.y':'hitPointOnObject.y_2',\n",
    "                                   'hitPointOnObject.z':'hitPointOnObject.z_2',\n",
    "                                   'hitObjectColliderBoundsCenter.x':'hitObjectColliderBoundsCenter.x_2',\n",
    "                                   'hitObjectColliderBoundsCenter.y':'hitObjectColliderBoundsCenter.y_2',\n",
    "                                   'hitObjectColliderBoundsCenter.z':'hitObjectColliderBoundsCenter.z_2'})\n",
    "                    combineDF = pd.concat([pdRC1, pdRC2], axis=1)\n",
    "                    combineDF.insert(len(combineDF.columns), 'DataRow',index)\n",
    "\n",
    "\n",
    "                else:\n",
    "                    print('!!!an exception occured in the ray cast data flattening in trial ', index)\n",
    "\n",
    "                # now add the new data row to the data overview\n",
    "                # rayCastData_df = [rayCastData_df]\n",
    "\n",
    "            \n",
    "                rayCastData_df.loc[index] = combineDF.loc[0]\n",
    "                \n",
    "            flatData_df = pd.concat([dataDF,rayCastData_df],axis=1)   \n",
    "\n",
    "            print('saving data')\n",
    "#             flatData_df.to_csv(PROCESSED_DATA_PATH + fileName[0:18] + '_flattened.csv', index = False)\n",
    "            flatData_df.to_csv(PROCESSED_DATA_PATH + fileName[0:12] + fileName[13:19] + '_flattened.csv', index = False)\n",
    "\n",
    "            print('data saved')\n",
    "            print('time is: ', time.ctime())\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 32,
   "id": "22b2107c",
=======
   "execution_count": 31,
   "id": "d5aed522",
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< Updated upstream
      "1754_Expl_S_1_ET_1\n"
=======
      "479\n"
>>>>>>> Stashed changes
     ]
    }
   ],
   "source": [
<<<<<<< Updated upstream
    "print( fileName[0:12] + fileName[13:19])"
=======
    "print(subject)"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f00a069",
   "metadata": {},
   "source": [
    "# ----------------- end of script ------------------------------ (old version below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86a5673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####################################### old Version ##############################################\n",
    "# # if no ray cast information is available, the data frame will be filled with nan values\n",
    "# # create empty data frames with nan values and correct variable names\n",
    "# columns1 = ['hitObjectColliderName_1','ordinalOfHit_1','hitPointOnObject.x_1','hitPointOnObject.y_1','hitPointOnObject.z_1',\n",
    "#             'hitObjectColliderBoundsCenter.x_1','hitObjectColliderBoundsCenter.y_1','hitObjectColliderBoundsCenter.z_1']\n",
    "\n",
    "# columns2 = ['hitObjectColliderName_2','ordinalOfHit_2','hitPointOnObject.x_2','hitPointOnObject.y_2','hitPointOnObject.z_2',\n",
    "#             'hitObjectColliderBoundsCenter.x_2','hitObjectColliderBoundsCenter.y_2','hitObjectColliderBoundsCenter.z_2']\n",
    "\n",
    "# columnsRCall = ['hitObjectColliderName_1','ordinalOfHit_1','hitPointOnObject.x_1','hitPointOnObject.y_1','hitPointOnObject.z_1',\n",
    "#                 'hitObjectColliderBoundsCenter.x_1','hitObjectColliderBoundsCenter.y_1','hitObjectColliderBoundsCenter.z_1',\n",
    "#                 'hitObjectColliderName_2','ordinalOfHit_2','hitPointOnObject.x_2','hitPointOnObject.y_2','hitPointOnObject.z_2',\n",
    "#                 'hitObjectColliderBoundsCenter.x_2','hitObjectColliderBoundsCenter.y_2','hitObjectColliderBoundsCenter.z_2','dataRow']\n",
    "\n",
    "# emptyDF1 = pd.DataFrame(np.nan,index=[0], columns= columns1)\n",
    "# emptyDF2 = pd.DataFrame(np.nan,index=[0], columns= columns2)\n",
    "\n",
    "\n",
    "# # create empty data overview data frame\n",
    "\n",
    "# flatData_df = pd.DataFrame(columns = allColumnNames)\n",
    "\n",
    "# # create empty subject information data frame\n",
    "\n",
    "# subjectData_df = pd.DataFrame(columns = infoVarNames)\n",
    "\n",
    "# #########################################################################################################\n",
    "# # data loop through all subjects and sessions\n",
    "\n",
    "# subcount = 0\n",
    "\n",
    "\n",
    "# for subject in subIDs:\n",
    "    \n",
    "#     subcount +=1\n",
    "#     print('Subject ' \n",
    "#           + str(subject) \n",
    "#           + ' started - ' \n",
    "#           + str(subcount) \n",
    "#           + '/' \n",
    "#           + str(len(subIDs)) \n",
    "#           + ' subjects')\n",
    "    \n",
    "# #     # Create empty dataframe for later concatenation\n",
    "# #     complete_exploration_df = pd.DataFrame(columns = col_names)\n",
    "# #     complete_exploration_df.head()\n",
    "    \n",
    "    \n",
    "#     # change dir into the subject folder \n",
    "#     CURRENT_SUBJECT_FOLDER = sorted([f for f in os.listdir(DATA_PATH+str(subject)) if not f.startswith('.')], key=str.lower)\n",
    "#     # get the data files according to the subject, ignoring OnQuit files\n",
    "#     subject_files = sorted([f for f in CURRENT_SUBJECT_FOLDER \n",
    "#                              if f.startswith(str(subject)+'_Expl_S_') and f.endswith(\"OnQuit.json\") == False], \n",
    "#                             key=str.lower) \n",
    "    \n",
    "#     # the following works as long as the data name format is as follows:\n",
    "#     # 'subjectID'_Expl_S_'SessionNumber'_ET_'EyeTrackingSessionNumber'_'UnixTimestamp'.json\n",
    "#     folder_files = list()\n",
    "    \n",
    "#     # loop through the subject folder and save all numbers\n",
    "#     for file in subject_files:\n",
    "#         folder_files.append(re.findall(r'\\d+', file))\n",
    "    \n",
    "#     # Extract all SubIDs (only one), SessionNumbers, ET_SessionNumbers (and Timestamps)\n",
    "#     try:\n",
    "#         SubID, SessionNumbers, ET_SessionNumbers, UnixTimestamp1, UnixTimeStamp2 = map(list, zip(*folder_files))\n",
    "#     except:\n",
    "#         print('\\tSubject ' \n",
    "#               + str(subject)\n",
    "#               + ' Filename is not valid!')\n",
    "        \n",
    "# #     print(SubID)\n",
    "# #     print(SessionNumbers)\n",
    "# #     print(ET_SessionNumbers)\n",
    "# #     print(UnixTimestamp1)\n",
    "# #     print(UnixTimeStamp2)\n",
    "    \n",
    "#     session_number = int(max(SessionNumbers)) # the maximum session number of the particular subject\n",
    "#     ET_session_number = int(max(ET_SessionNumbers)) # the maximum ET session number of the particular subject\n",
    "    \n",
    "    \n",
    "#     # print info of how many files were found \n",
    "    \n",
    "#     print(len(SubID), ' files were found for participant ', SubID[0])\n",
    "#     print('A maximum of ', session_number, 'sessions were found and will be processed')\n",
    "        \n",
    "# # --------- second layer - exploration session loop ---------\n",
    "\n",
    "#     # loop over exploration sessions\n",
    "#     for EXP_session in range(session_number):\n",
    "#         # to avoid start at 0\n",
    "#         EXP_session +=1 \n",
    "\n",
    "#         # extract the exploration data files for each session - but exclude OnQuit files\n",
    "#         subject_data = sorted([f for f in CURRENT_SUBJECT_FOLDER if f.startswith(str(subject) + '_Expl_S_' + str(EXP_session)) \n",
    "#                                and f.endswith(\"OnQuit.json\") == False], key=str.lower)\n",
    "\n",
    "\n",
    "#         print(\"\\tTotal Sessionfiles: \"\n",
    "#               + str(len(subject_data))\n",
    "#               + \" - Exploration Session \"\n",
    "#               + str(EXP_session))\n",
    "\n",
    "#         ET_session_count = 0 # session count\n",
    "\n",
    "# # --------- third layer - eye tracking session loop ---------\n",
    "\n",
    "#         # loop over separate eye tracking sessions\n",
    "#         for fileName in subject_data:\n",
    "#             ET_session_count+=1\n",
    "\n",
    "#             print('load data of file ', fileName)\n",
    "\n",
    "#             print('Path: ', DATA_PATH + str(subject) + '/' + fileName)\n",
    "#             # open the JSON file as dictionary\n",
    "#             with open(DATA_PATH + str(subject) + '/' + fileName) as datafile:\n",
    "#                 try:\n",
    "#                     print(\"read file\")\n",
    "#                     dataR = '['+ datafile.read()\n",
    "#                     dataR = dataR[:len(dataR)] + \"]\"\n",
    "#                 except:\n",
    "#                     print(\"reading did not work\")\n",
    "\n",
    "#                 subject_session = json.loads(dataR)\n",
    "#                 print(\" data loaded\")\n",
    "\n",
    "\n",
    "\n",
    "# ##################################################################################################################\n",
    "\n",
    "# #                 # Data flattening part: \n",
    "# #                 # first save trial information\n",
    "\n",
    "# #                 currentTrialInfo = pd.json_normalize(subject_session[0]['trials'][0])\n",
    "# #                 currentTrialInfo = infoDF.drop(columns=['dataPoints'])\n",
    "# #                 currentTrialInfo.insert(0,'Participant',subject)\n",
    "# #                 currentTrialInfo.insert(1,'Session',ET_session)\n",
    "\n",
    "# #                 subjectData_df = pd.concat[subjectData_df, currentTrialInfo]\n",
    "\n",
    "\n",
    "#             # create empty data overview data frame\n",
    "#             flatData_df = pd.DataFrame(columns = allColumnNames)\n",
    "        \n",
    "#             # flatten the majority of the variables into currentDF data frame\n",
    "#             #currentDF_raw = pd.json_normalize(subject_session[0]['trials'][0]['dataPoints'])\n",
    "\n",
    "#             # remove the 'rayCastHitsCombinedEyes' column as it still contains a nested data structure\n",
    "#             #currentDF = currentDF_raw.drop(columns=['rayCastHitsCombinedEyes'])\n",
    "            \n",
    "#             # create an empty data frame of the required size\n",
    "#             #rayCastData_df = pd.DataFrame(np.nan,index=len(subject_session[0]['trials'][0]['dataPoints'], columns= columnsRCall)\n",
    "\n",
    "            \n",
    "#             start = timer()\n",
    "#             # now loop through the individual trials and flatten the data\n",
    "#             for index in range(20):#range(len(subject_session[0]['trials'][0]['dataPoints'])):\n",
    "                \n",
    "#                 # flatten the majority of the variables into currentDF data frame\n",
    "#                 currentDF_raw = pd.json_normalize(subject_session[0]['trials'][0]['dataPoints'][index])\n",
    "\n",
    "#                 # remove the 'rayCastHitsCombinedEyes' column as it still contains a nested data structure\n",
    "#                 currentDF = currentDF_raw.drop(columns=['rayCastHitsCombinedEyes'])\n",
    "            \n",
    "                \n",
    "#                 # depending on the size of the ray cast data - flatten data and appand it to currentDF data frame\n",
    "#                 # the variables are renamed to make the differentiation of first and second order collider hits more intuitive\n",
    "#                 #lengthRCData = len(subject_session[0]['trials'][0]['dataPoints'][index]['rayCastHitsCombinedEyes'][0])\n",
    "#                 lengthRCData = len(currentDF_raw['rayCastHitsCombinedEyes'][0])\n",
    "                \n",
    "#                 if lengthRCData ==0: #case: no ray cast data is available = no collider was hit\n",
    "\n",
    "#                     combineDF = pd.concat([currentDF, emptyDF1, emptyDF2], axis=1)\n",
    "#                     combineDF.insert(len(combineDF.columns), 'dataRow',index)\n",
    "\n",
    "\n",
    "#                 elif lengthRCData == 1: # case: only one collider was hit, there is no secondary hit\n",
    "\n",
    "#                     pdRC1= pd.json_normalize(currentDF_raw['rayCastHitsCombinedEyes'][0][0]).rename(\n",
    "#                         columns = {'hitObjectColliderName':'hitObjectColliderName_1',\n",
    "#                                    'ordinalOfHit':'ordinalOfHit_1',\n",
    "#                                    'hitPointOnObject.x':'hitPointOnObject.x_1',\n",
    "#                                    'hitPointOnObject.y':'hitPointOnObject.y_1',\n",
    "#                                    'hitPointOnObject.z':'hitPointOnObject.z_1',\n",
    "#                                    'hitObjectColliderBoundsCenter.x':'hitObjectColliderBoundsCenter.x_1',\n",
    "#                                    'hitObjectColliderBoundsCenter.y':'hitObjectColliderBoundsCenter.y_1',\n",
    "#                                    'hitObjectColliderBoundsCenter.z':'hitObjectColliderBoundsCenter.z_1'})\n",
    "#                     combineDF = pd.concat([currentDF, pdRC1, emptyDF2], axis=1)\n",
    "#                     combineDF.insert(len(combineDF.columns), 'dataRow',index)\n",
    "\n",
    "#                 elif lengthRCData == 2: # case: two collider were hit \n",
    "\n",
    "#                     pdRC1= pd.json_normalize(currentDF_raw['rayCastHitsCombinedEyes'][0][0]).rename(\n",
    "#                         columns = {'hitObjectColliderName':'hitObjectColliderName_1',\n",
    "#                                    'ordinalOfHit':'ordinalOfHit_1',\n",
    "#                                    'hitPointOnObject.x':'hitPointOnObject.x_1',\n",
    "#                                    'hitPointOnObject.y':'hitPointOnObject.y_1',\n",
    "#                                    'hitPointOnObject.z':'hitPointOnObject.z_1',\n",
    "#                                    'hitObjectColliderBoundsCenter.x':'hitObjectColliderBoundsCenter.x_1',\n",
    "#                                    'hitObjectColliderBoundsCenter.y':'hitObjectColliderBoundsCenter.y_1',\n",
    "#                                    'hitObjectColliderBoundsCenter.z':'hitObjectColliderBoundsCenter.z_1'})\n",
    "\n",
    "#                     pdRC2 = pd.json_normalize(currentDF_raw['rayCastHitsCombinedEyes'][0][1]).rename(\n",
    "#                         columns = {'hitObjectColliderName':'hitObjectColliderName_2',\n",
    "#                                    'ordinalOfHit':'ordinalOfHit_2',\n",
    "#                                    'hitPointOnObject.x':'hitPointOnObject.x_2',\n",
    "#                                    'hitPointOnObject.y':'hitPointOnObject.y_2',\n",
    "#                                    'hitPointOnObject.z':'hitPointOnObject.z_2',\n",
    "#                                    'hitObjectColliderBoundsCenter.x':'hitObjectColliderBoundsCenter.x_2',\n",
    "#                                    'hitObjectColliderBoundsCenter.y':'hitObjectColliderBoundsCenter.y_2',\n",
    "#                                    'hitObjectColliderBoundsCenter.z':'hitObjectColliderBoundsCenter.z_2'})\n",
    "#                     combineDF = pd.concat([currentDF, pdRC1, pdRC2], axis=1)\n",
    "#                     combineDF.insert(len(combineDF.columns), 'dataRow',index)\n",
    "\n",
    "\n",
    "#                 else:\n",
    "#                     print('!!!an exception occured in the ray cast data flattening in trial ', index)\n",
    "\n",
    "#                 # now add the new data row to the data overview\n",
    "#                 # rayCastData_df = [rayCastData_df]\n",
    "\n",
    "            \n",
    "#                 flatData_df = pd.concat([flatData_df,combineDF],ignore_index=True)\n",
    "\n",
    "#             print('saving data')\n",
    "#             flatData_df.to_csv(PROCESSED_DATA_PATH + fileName[0:18] + '_flattened.csv', index = False)\n",
    "#             print('data saved')\n",
    "#             end= timer () \n",
    "#             print('timer:', end-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c4bd23aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-10.002787431081137"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1625559186.596578 - 1625559186.5970739\n",
    "# (1625559571.5704663- 1625559441.909445)/60\n",
    "\n",
    "# (1625558563.1912049 - 1625559186.596578) / 60\n",
    "# (1625559163.3901957 - 1625559441.909445)/60\n",
    "# on quit start - end\n",
    "# (1625559186.596578 - 1625559441.909445)/ 60\n",
    "\n",
    "# normales file start - end\n",
    "\n",
    "# (1625558563.1917012 - 1625559163.358947)/60\n",
    "\n",
    "# ende on quite 3 - file 4\n",
    "1625559163.358947 - 1625559757.8188573"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbf8583",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 879.60078,
   "position": {
    "height": "901.823px",
    "left": "1593px",
    "right": "20px",
    "top": "115px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
