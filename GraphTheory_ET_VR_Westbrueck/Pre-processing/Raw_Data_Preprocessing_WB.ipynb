{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "050dead6",
   "metadata": {},
   "source": [
    "# Raw_Data_Preprocessing_WB\n",
    "- written by Jasmin L. Walter (jawalter@uos.de)\n",
    "- reads in nested json files and returns flattened csv files\n",
    "- does not change anything in the data, only extracts all variables from all 9 layers of the nested json files and saves them in data frames / csv files unchanged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933ccf61-dde6-4454-81ac-72aec97a2cb1",
   "metadata": {},
   "source": [
    "Note: this script is optimized to run with the raw json recordings from the SpaRe Westbrook builds uploaded here: https://osf.io/32sqe/overview\n",
    "\n",
    "When working with the \"Dataset: eye and motion tracking in immersive VR (urban city environment Westbrook)\" uploaded here: https://osf.io/32sqe, skip this script, since the uploaded data is the output of this (very time intensive) processing step, that does not change anything in the data but just unflattens the nested json structure. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41446f1-a888-4957-ad25-05e999737c85",
   "metadata": {},
   "source": [
    "Purpose: Flatten nested Unity VR eye-tracking JSON logs into per-file CSVs without altering values.\n",
    "Extracts all variables (incl. up to two ray-cast hits per sample) from the nested structure\n",
    "and writes a flattened data frame per recording, plus a trial info summary.\n",
    "Usage:\n",
    "\n",
    "Set DATA_PATH and PROCESSED_DATA_PATH to your raw JSON folder and output folder.\n",
    "Optionally set subIDs to restrict processing; otherwise participants are inferred from DATA_PATH.\n",
    "Run in Python. Inputs (expected JSON naming and structure):\n",
    "Files named like <ParticipantID>Expl_S<Session>ET<ETSession>_<UnixTS>.json (OnQuit.json files are ignored)\n",
    "JSON structure must contain trials[0]['dataPoints'] with 'rayCastHitsCombinedEyes' Outputs (to PROCESSED_DATA_PATH):\n",
    "<prefix>_infoSummary.csv trial-level metadata (no dataPoints), adds FileInfo\n",
    "<prefix>_flattened.csv flattened dataPoints with columns for first/second ray-cast hits; adds 'DataRow' indexing the original row Notes:\n",
    "Ray-cast hits: rows with 0 hits are filled with NaNs (correct column schema); 1 hit fills “_1”, 2 hits fill “_1” and “_2” columns.\n",
    "Files are discovered and sorted per participant and session; OnQuit.json files are excluded. Dependencies:\n",
    "Python >= 3.9\n",
    "pandas, numpy (json, re, os, time, warnings from the standard library) License: GNU General Public License v3.0 (GPL-3.0) (see LICENSE) \"\"\"\n",
    "\n",
    "\n",
    "Note, not all imported functions might be required \n",
    "\n",
    "Required by the current code:\n",
    "\n",
    "os (paths, listing)\n",
    "json (loading JSON)\n",
    "numpy as np (np.nan, unique)\n",
    "re (extracting numeric tokens from filenames)\n",
    "pandas as pd (json_normalize, DataFrame ops, CSV I/O)\n",
    "time (time.ctime for logging)\n",
    "warnings (suppress FutureWarning)\n",
    "\n",
    "\n",
    "Potentially save to remove (but double check whether script keeps running):\n",
    "\n",
    "importlib, check_package, StopExecution (not invoked)\n",
    "cv2\n",
    "matplotlib.pyplot as plt\n",
    "glob\n",
    "scipy.cluster.vq as clusters\n",
    "pandas.plotting.autocorrelation_plot as AC_plot\n",
    "statsmodels.graphics.tsaplots\n",
    "statsmodels.tsa.stattools.acf\n",
    "mpl_toolkits.mplot3d.Axes3D\n",
    "matplotlib.colors.LinearSegmentedColormap\n",
    "from timeit import default_timer as timer\n",
    "numpy imports/aliases already covered; networkx, skimage, sklearn are commented out (fine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfa87cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General configuration\n",
    "import os\n",
    "\n",
    "# install_missing_packages: bool\n",
    "#     A flag indicating if missing packages should be automatically installed\n",
    "install_missing_packages = True\n",
    "\n",
    "# use_conda: bool\n",
    "#     A flag indicating if conda should be used for software installation.\n",
    "#     If False, pip will be used. The default is to use conda if jupyter\n",
    "#     is run in a conda environment.\n",
    "use_conda = 'CONDA_EXE' in os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baed198d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "def check_package(package, pip_pkg: str = None, conda_pkg: str = None):\n",
    "    \"\"\"Check if a given package is installed. If missing install\n",
    "    it (if global flag `install_missing_packages` is True) either with\n",
    "    pip or with conda (depending on `use_conda`).\n",
    "    \"\"\"\n",
    "    if importlib.util.find_spec(package) is not None:\n",
    "        return  # ok, package is already installed\n",
    "\n",
    "    if not install_missing_packages:\n",
    "        raise RuntimeError(f\"{package} is not installed!\")\n",
    "\n",
    "    if use_conda:\n",
    "        import conda.cli\n",
    "        conda.cli.main('conda', 'install',  '-y', conda_pkg or package)\n",
    "    else:\n",
    "        import subprocess\n",
    "        import sys            \n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', pip_pkg or package])\n",
    "        \n",
    "# This is to exit cells without error tracebacks (cosmetic purpose)\n",
    "class StopExecution(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f10e4b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "#import networkx as nx\n",
    "import glob\n",
    "import scipy.cluster.vq as clusters\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "#from sklearn.preprocessing import normalize\n",
    "from pandas.plotting import autocorrelation_plot as AC_plot \n",
    "from statsmodels.graphics import tsaplots\n",
    "from statsmodels.tsa.stattools import acf\n",
    "#from skimage.filters import gaussian\n",
    "from mpl_toolkits.mplot3d import Axes3D \n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ad0097",
   "metadata": {},
   "source": [
    "# Customize to run scripts - paths, subject ids to run etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2c98379",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'D:/Studium/PhD/Westbrueck/Data/raw/'\n",
    "\n",
    "PROCESSED_DATA_PATH = 'D:/Studium/PhD/Westbrueck/Data/pre-processed/'\n",
    "\n",
    "# Getting the Folder without hidden files in ascending order \n",
    "DATA_FOLDER = sorted([f for f in os.listdir(DATA_PATH) if not f.startswith('.')], key=str.lower)\n",
    "PROCESSED_DATA_FOLDER = sorted([f for f in os.listdir(PROCESSED_DATA_PATH) if not f.startswith('.')], key=str.lower)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53292cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1004 1005]\n"
     ]
    }
   ],
   "source": [
    "subIDs = []\n",
    "for sub in DATA_FOLDER:\n",
    "    if sub[0:4].isdigit() and sub.startswith('1'):\n",
    "        subIDs.append(int(sub[0:4]))\n",
    "    else:\n",
    "        pass\n",
    "subIDs = np.unique(subIDs)\n",
    "print(subIDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aed9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # custom subIDs, if only a subset of participants should be processed\n",
    "# subIDs = [1004]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0788e108",
   "metadata": {},
   "source": [
    "# Main part - flatten all nested data structures and save as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89e8ac41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 1004 started - 1/2 subjects\n",
      "15  files were found for participant  1004\n",
      "A maximum of  5 sessions were found and will be processed\n",
      "\tTotal Sessionfiles: 3 - Exploration Session 1\n",
      "load data of file  1004_Expl_S_1_ET_1_1618996584.88327.json\n",
      "Path:  D:/Studium/PhD/Westbrueck/Data/raw/1004/1004_Expl_S_1_ET_1_1618996584.88327.json\n",
      "read file\n",
      "data loaded\n",
      "time is:  Tue Jul  5 10:30:25 2022\n",
      "trial info saved\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-acf79216deec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m                 \u001b[0mrayCastData_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcombineDF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m             \u001b[0mflatData_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdataDF\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrayCastData_df\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Software\\Programme\\Anaconda3\\envs\\graphs\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    668\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m         \u001b[0miloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"iloc\"\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 670\u001b[1;33m         \u001b[0miloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    671\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Software\\Programme\\Anaconda3\\envs\\graphs\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[1;34m(self, indexer, value)\u001b[0m\n\u001b[0;32m   1756\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1757\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0milocs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1758\u001b[1;33m                         \u001b[0misetter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1759\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1760\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Software\\Programme\\Anaconda3\\envs\\graphs\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36misetter\u001b[1;34m(loc, v)\u001b[0m\n\u001b[0;32m   1686\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1687\u001b[0m                     \u001b[1;31m# set the item, possibly having a dtype change\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1688\u001b[1;33m                     \u001b[0mser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1689\u001b[0m                     \u001b[0mser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mplane_indexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1690\u001b[0m                     \u001b[0mser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_update_cacher\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclear\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Software\\Programme\\Anaconda3\\envs\\graphs\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mcopy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m   5663\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5664\u001b[0m         \"\"\"\n\u001b[1;32m-> 5665\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5666\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5667\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"copy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Software\\Programme\\Anaconda3\\envs\\graphs\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mcopy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mnew_axes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"copy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m         \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_axes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Software\\Programme\\Anaconda3\\envs\\graphs\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[0;32m    407\u001b[0m                 \u001b[0mapplied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 409\u001b[1;33m                 \u001b[0mapplied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    410\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Software\\Programme\\Anaconda3\\envs\\graphs\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\u001b[0m in \u001b[0;36mcopy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    677\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 679\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    680\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_block_same_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    681\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# if no ray cast information is available, the data frame will be filled with nan values\n",
    "# create empty data frames with nan values and correct variable names\n",
    "columns1 = ['hitObjectColliderName_1','ordinalOfHit_1','hitPointOnObject.x_1','hitPointOnObject.y_1','hitPointOnObject.z_1',\n",
    "            'hitObjectColliderBoundsCenter.x_1','hitObjectColliderBoundsCenter.y_1','hitObjectColliderBoundsCenter.z_1']\n",
    "\n",
    "columns2 = ['hitObjectColliderName_2','ordinalOfHit_2','hitPointOnObject.x_2','hitPointOnObject.y_2','hitPointOnObject.z_2',\n",
    "            'hitObjectColliderBoundsCenter.x_2','hitObjectColliderBoundsCenter.y_2','hitObjectColliderBoundsCenter.z_2']\n",
    "\n",
    "columnsRCall = ['hitObjectColliderName_1','ordinalOfHit_1','hitPointOnObject.x_1','hitPointOnObject.y_1','hitPointOnObject.z_1',\n",
    "                'hitObjectColliderBoundsCenter.x_1','hitObjectColliderBoundsCenter.y_1','hitObjectColliderBoundsCenter.z_1',\n",
    "                'hitObjectColliderName_2','ordinalOfHit_2','hitPointOnObject.x_2','hitPointOnObject.y_2','hitPointOnObject.z_2',\n",
    "                'hitObjectColliderBoundsCenter.x_2','hitObjectColliderBoundsCenter.y_2','hitObjectColliderBoundsCenter.z_2',\n",
    "                'DataRow']\n",
    "\n",
    "emptyDF1 = pd.DataFrame(np.nan,index=[0], columns= columns1)\n",
    "emptyDF2 = pd.DataFrame(np.nan,index=[0], columns= columns2)\n",
    "\n",
    "\n",
    "\n",
    "#########################################################################################################\n",
    "# data loop through all subjects and sessions\n",
    "\n",
    "subcount = 0\n",
    "\n",
    "\n",
    "for subject in subIDs:\n",
    "    \n",
    "    subcount +=1\n",
    "    print('Subject ' \n",
    "          + str(subject) \n",
    "          + ' started - ' \n",
    "          + str(subcount) \n",
    "          + '/' \n",
    "          + str(len(subIDs)) \n",
    "          + ' subjects')\n",
    "    \n",
    "#     # Create empty dataframe for later concatenation\n",
    "# complete_exploration_df = pd.DataFrame(columns = col_names)\n",
    "#     complete_exploration_df.head()\n",
    "    \n",
    "    \n",
    "    # change dir into the subject folder \n",
    "    CURRENT_SUBJECT_FOLDER = sorted([f for f in os.listdir(DATA_PATH+str(subject)) if not f.startswith('.')], key=str.lower)\n",
    "    # get the data files according to the subject, ignoring OnQuit files\n",
    "    subject_files = sorted([f for f in CURRENT_SUBJECT_FOLDER \n",
    "                             if f.startswith(str(subject)+'_Expl_S_') and f.endswith(\"OnQuit.json\") == False], \n",
    "                            key=str.lower) \n",
    "    \n",
    "    # the following works as long as the data name format is as follows:\n",
    "    # 'subjectID'_Expl_S_'SessionNumber'_ET_'EyeTrackingSessionNumber'_'UnixTimestamp'.json\n",
    "    folder_files = list()\n",
    "    \n",
    "    # loop through the subject folder and save all numbers\n",
    "    for file in subject_files:\n",
    "        folder_files.append(re.findall(r'\\d+', file))\n",
    "    \n",
    "    # Extract all SubIDs (only one), SessionNumbers, ET_SessionNumbers (and Timestamps)\n",
    "    try:\n",
    "        SubID, SessionNumbers, ET_SessionNumbers, UnixTimestamp1, UnixTimeStamp2 = map(list, zip(*folder_files))\n",
    "    except:\n",
    "        print('\\tSubject ' \n",
    "              + str(subject)\n",
    "              + ' Filename is not valid!')\n",
    "        \n",
    "#     print(SubID)\n",
    "#     print(SessionNumbers)\n",
    "#     print(ET_SessionNumbers)\n",
    "#     print(UnixTimestamp1)\n",
    "#     print(UnixTimeStamp2)\n",
    "    \n",
    "    session_number = int(max(SessionNumbers)) # the maximum session number of the particular subject\n",
    "    ET_session_number = int(max(ET_SessionNumbers)) # the maximum ET session number of the particular subject\n",
    "    \n",
    "    \n",
    "    # print info of how many files were found \n",
    "    \n",
    "    print(len(SubID), ' files were found for participant ', SubID[0])\n",
    "    print('A maximum of ', session_number, 'sessions were found and will be processed')\n",
    "        \n",
    "# --------- second layer - exploration session loop ---------\n",
    "\n",
    "    # loop over exploration sessions\n",
    "    for EXP_session in range(session_number):\n",
    "        # to avoid start at 0\n",
    "        EXP_session +=1 \n",
    "\n",
    "        # extract the exploration data files for each session - but exclude OnQuit files\n",
    "        subject_data = sorted([f for f in CURRENT_SUBJECT_FOLDER if f.startswith(str(subject) + '_Expl_S_' + str(EXP_session)) \n",
    "                               and f.endswith(\"OnQuit.json\") == False], key=str.lower)\n",
    "\n",
    "\n",
    "        print(\"\\tTotal Sessionfiles: \"\n",
    "              + str(len(subject_data))\n",
    "              + \" - Exploration Session \"\n",
    "              + str(EXP_session))\n",
    "\n",
    "        ET_session_count = 0 # session count\n",
    "\n",
    "# --------- third layer - eye tracking session loop ---------\n",
    "\n",
    "        # loop over separate eye tracking sessions\n",
    "        for fileName in subject_data:\n",
    "            ET_session_count+=1\n",
    "\n",
    "            print('load data of file ', fileName)\n",
    "\n",
    "            print('Path: ', DATA_PATH + str(subject) + '/' + fileName)\n",
    "            # open the JSON file as dictionary\n",
    "            with open(DATA_PATH + str(subject) + '/' + fileName) as datafile:\n",
    "                try:\n",
    "                    print(\"read file\")\n",
    "                    dataR = '['+ datafile.read()\n",
    "                    dataR = dataR[:len(dataR)] + \"]\"\n",
    "                except:\n",
    "                    print(\"reading did not work\")\n",
    "\n",
    "                subject_session = json.loads(dataR)\n",
    "                print(\"data loaded\")\n",
    "                print('time is: ', time.ctime())\n",
    "\n",
    "\n",
    "\n",
    "##################################################################################################################\n",
    "\n",
    "            # Data flattening part: \n",
    "            # first save the overall trial information\n",
    "\n",
    "\n",
    "            infoDF = pd.json_normalize(subject_session[0]['trials'][0])\n",
    "            infoDF = infoDF.drop(columns=['dataPoints'])\n",
    "            infoDF.insert(0,'FileInfo',fileName[0:18])\n",
    "            infoDF.to_csv(PROCESSED_DATA_PATH + fileName[0:18] + '_infoSummary.csv', index = False)\n",
    "            print('trial info saved')\n",
    "            \n",
    "\n",
    "            # flatten the majority of the variables into currentDF data frame\n",
    "            currentDF_raw = pd.json_normalize(subject_session[0]['trials'][0]['dataPoints'])\n",
    "\n",
    "            # remove the 'rayCastHitsCombinedEyes' column as it still contains a nested data structure\n",
    "            dataDF = currentDF_raw.drop(columns=['rayCastHitsCombinedEyes'])\n",
    "            \n",
    "            # create an empty data frame of the required size\n",
    "            rayCastData_df = pd.DataFrame(np.nan,index=range(len(subject_session[0]['trials'][0]['dataPoints'])), columns= columnsRCall)\n",
    "\n",
    "            # now loop through the individual trials and flatten the data\n",
    "            for index in range(len(subject_session[0]['trials'][0]['dataPoints'])):\n",
    "                \n",
    "                # depending on the size of the ray cast data - flatten data and appand it to currentDF data frame\n",
    "                # the variables are renamed to make the differentiation of first and second order collider hits more intuitive\n",
    "                #lengthRCData = len(subject_session[0]['trials'][0]['dataPoints'][index]['rayCastHitsCombinedEyes'][0])\n",
    "                lengthRCData = len(currentDF_raw.at[index,'rayCastHitsCombinedEyes'])\n",
    "                \n",
    "                \n",
    "                if lengthRCData ==0: #case: no ray cast data is available = no collider was hit\n",
    "\n",
    "                    combineDF = pd.concat([emptyDF1, emptyDF2], axis=1)\n",
    "                    combineDF.insert(len(combineDF.columns), 'DataRow',index)\n",
    "\n",
    "\n",
    "                elif lengthRCData == 1: # case: only one collider was hit, there is no secondary hit\n",
    "\n",
    "                    pdRC1= pd.json_normalize(currentDF_raw.at[index,'rayCastHitsCombinedEyes'][0]).rename(\n",
    "                        columns = {'hitObjectColliderName':'hitObjectColliderName_1',\n",
    "                                   'ordinalOfHit':'ordinalOfHit_1',\n",
    "                                   'hitPointOnObject.x':'hitPointOnObject.x_1',\n",
    "                                   'hitPointOnObject.y':'hitPointOnObject.y_1',\n",
    "                                   'hitPointOnObject.z':'hitPointOnObject.z_1',\n",
    "                                   'hitObjectColliderBoundsCenter.x':'hitObjectColliderBoundsCenter.x_1',\n",
    "                                   'hitObjectColliderBoundsCenter.y':'hitObjectColliderBoundsCenter.y_1',\n",
    "                                   'hitObjectColliderBoundsCenter.z':'hitObjectColliderBoundsCenter.z_1'})\n",
    "                    combineDF = pd.concat([pdRC1, emptyDF2], axis=1)\n",
    "                    combineDF.insert(len(combineDF.columns), 'DataRow',index)\n",
    "\n",
    "                elif lengthRCData == 2: # case: two collider were hit \n",
    "\n",
    "                    pdRC1= pd.json_normalize(currentDF_raw.at[index,'rayCastHitsCombinedEyes'][0]).rename(\n",
    "                        columns = {'hitObjectColliderName':'hitObjectColliderName_1',\n",
    "                                   'ordinalOfHit':'ordinalOfHit_1',\n",
    "                                   'hitPointOnObject.x':'hitPointOnObject.x_1',\n",
    "                                   'hitPointOnObject.y':'hitPointOnObject.y_1',\n",
    "                                   'hitPointOnObject.z':'hitPointOnObject.z_1',\n",
    "                                   'hitObjectColliderBoundsCenter.x':'hitObjectColliderBoundsCenter.x_1',\n",
    "                                   'hitObjectColliderBoundsCenter.y':'hitObjectColliderBoundsCenter.y_1',\n",
    "                                   'hitObjectColliderBoundsCenter.z':'hitObjectColliderBoundsCenter.z_1'})\n",
    "\n",
    "                    pdRC2 = pd.json_normalize(currentDF_raw.at[index,'rayCastHitsCombinedEyes'][1]).rename(\n",
    "                        columns = {'hitObjectColliderName':'hitObjectColliderName_2',\n",
    "                                   'ordinalOfHit':'ordinalOfHit_2',\n",
    "                                   'hitPointOnObject.x':'hitPointOnObject.x_2',\n",
    "                                   'hitPointOnObject.y':'hitPointOnObject.y_2',\n",
    "                                   'hitPointOnObject.z':'hitPointOnObject.z_2',\n",
    "                                   'hitObjectColliderBoundsCenter.x':'hitObjectColliderBoundsCenter.x_2',\n",
    "                                   'hitObjectColliderBoundsCenter.y':'hitObjectColliderBoundsCenter.y_2',\n",
    "                                   'hitObjectColliderBoundsCenter.z':'hitObjectColliderBoundsCenter.z_2'})\n",
    "                    combineDF = pd.concat([pdRC1, pdRC2], axis=1)\n",
    "                    combineDF.insert(len(combineDF.columns), 'DataRow',index)\n",
    "\n",
    "\n",
    "                else:\n",
    "                    print('!!!an exception occured in the ray cast data flattening in trial ', index)\n",
    "\n",
    "                # now add the new data row to the data overview\n",
    "                # rayCastData_df = [rayCastData_df]\n",
    "\n",
    "            \n",
    "                rayCastData_df.loc[index] = combineDF.loc[0]\n",
    "                \n",
    "            flatData_df = pd.concat([dataDF,rayCastData_df],axis=1)   \n",
    "\n",
    "            print('saving data')\n",
    "            flatData_df.to_csv(PROCESSED_DATA_PATH + fileName[0:18] + '_flattened.csv', index = False)\n",
    "            print('data saved')\n",
    "            print('time is: ', time.ctime())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f00a069",
   "metadata": {},
   "source": [
    "# ----------------- end of script ------------------------------ (old version below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86a5673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####################################### old Version ##############################################\n",
    "# # if no ray cast information is available, the data frame will be filled with nan values\n",
    "# # create empty data frames with nan values and correct variable names\n",
    "# columns1 = ['hitObjectColliderName_1','ordinalOfHit_1','hitPointOnObject.x_1','hitPointOnObject.y_1','hitPointOnObject.z_1',\n",
    "#             'hitObjectColliderBoundsCenter.x_1','hitObjectColliderBoundsCenter.y_1','hitObjectColliderBoundsCenter.z_1']\n",
    "\n",
    "# columns2 = ['hitObjectColliderName_2','ordinalOfHit_2','hitPointOnObject.x_2','hitPointOnObject.y_2','hitPointOnObject.z_2',\n",
    "#             'hitObjectColliderBoundsCenter.x_2','hitObjectColliderBoundsCenter.y_2','hitObjectColliderBoundsCenter.z_2']\n",
    "\n",
    "# columnsRCall = ['hitObjectColliderName_1','ordinalOfHit_1','hitPointOnObject.x_1','hitPointOnObject.y_1','hitPointOnObject.z_1',\n",
    "#                 'hitObjectColliderBoundsCenter.x_1','hitObjectColliderBoundsCenter.y_1','hitObjectColliderBoundsCenter.z_1',\n",
    "#                 'hitObjectColliderName_2','ordinalOfHit_2','hitPointOnObject.x_2','hitPointOnObject.y_2','hitPointOnObject.z_2',\n",
    "#                 'hitObjectColliderBoundsCenter.x_2','hitObjectColliderBoundsCenter.y_2','hitObjectColliderBoundsCenter.z_2','dataRow']\n",
    "\n",
    "# emptyDF1 = pd.DataFrame(np.nan,index=[0], columns= columns1)\n",
    "# emptyDF2 = pd.DataFrame(np.nan,index=[0], columns= columns2)\n",
    "\n",
    "\n",
    "# # create empty data overview data frame\n",
    "\n",
    "# flatData_df = pd.DataFrame(columns = allColumnNames)\n",
    "\n",
    "# # create empty subject information data frame\n",
    "\n",
    "# subjectData_df = pd.DataFrame(columns = infoVarNames)\n",
    "\n",
    "# #########################################################################################################\n",
    "# # data loop through all subjects and sessions\n",
    "\n",
    "# subcount = 0\n",
    "\n",
    "\n",
    "# for subject in subIDs:\n",
    "    \n",
    "#     subcount +=1\n",
    "#     print('Subject ' \n",
    "#           + str(subject) \n",
    "#           + ' started - ' \n",
    "#           + str(subcount) \n",
    "#           + '/' \n",
    "#           + str(len(subIDs)) \n",
    "#           + ' subjects')\n",
    "    \n",
    "# #     # Create empty dataframe for later concatenation\n",
    "# #     complete_exploration_df = pd.DataFrame(columns = col_names)\n",
    "# #     complete_exploration_df.head()\n",
    "    \n",
    "    \n",
    "#     # change dir into the subject folder \n",
    "#     CURRENT_SUBJECT_FOLDER = sorted([f for f in os.listdir(DATA_PATH+str(subject)) if not f.startswith('.')], key=str.lower)\n",
    "#     # get the data files according to the subject, ignoring OnQuit files\n",
    "#     subject_files = sorted([f for f in CURRENT_SUBJECT_FOLDER \n",
    "#                              if f.startswith(str(subject)+'_Expl_S_') and f.endswith(\"OnQuit.json\") == False], \n",
    "#                             key=str.lower) \n",
    "    \n",
    "#     # the following works as long as the data name format is as follows:\n",
    "#     # 'subjectID'_Expl_S_'SessionNumber'_ET_'EyeTrackingSessionNumber'_'UnixTimestamp'.json\n",
    "#     folder_files = list()\n",
    "    \n",
    "#     # loop through the subject folder and save all numbers\n",
    "#     for file in subject_files:\n",
    "#         folder_files.append(re.findall(r'\\d+', file))\n",
    "    \n",
    "#     # Extract all SubIDs (only one), SessionNumbers, ET_SessionNumbers (and Timestamps)\n",
    "#     try:\n",
    "#         SubID, SessionNumbers, ET_SessionNumbers, UnixTimestamp1, UnixTimeStamp2 = map(list, zip(*folder_files))\n",
    "#     except:\n",
    "#         print('\\tSubject ' \n",
    "#               + str(subject)\n",
    "#               + ' Filename is not valid!')\n",
    "        \n",
    "# #     print(SubID)\n",
    "# #     print(SessionNumbers)\n",
    "# #     print(ET_SessionNumbers)\n",
    "# #     print(UnixTimestamp1)\n",
    "# #     print(UnixTimeStamp2)\n",
    "    \n",
    "#     session_number = int(max(SessionNumbers)) # the maximum session number of the particular subject\n",
    "#     ET_session_number = int(max(ET_SessionNumbers)) # the maximum ET session number of the particular subject\n",
    "    \n",
    "    \n",
    "#     # print info of how many files were found \n",
    "    \n",
    "#     print(len(SubID), ' files were found for participant ', SubID[0])\n",
    "#     print('A maximum of ', session_number, 'sessions were found and will be processed')\n",
    "        \n",
    "# # --------- second layer - exploration session loop ---------\n",
    "\n",
    "#     # loop over exploration sessions\n",
    "#     for EXP_session in range(session_number):\n",
    "#         # to avoid start at 0\n",
    "#         EXP_session +=1 \n",
    "\n",
    "#         # extract the exploration data files for each session - but exclude OnQuit files\n",
    "#         subject_data = sorted([f for f in CURRENT_SUBJECT_FOLDER if f.startswith(str(subject) + '_Expl_S_' + str(EXP_session)) \n",
    "#                                and f.endswith(\"OnQuit.json\") == False], key=str.lower)\n",
    "\n",
    "\n",
    "#         print(\"\\tTotal Sessionfiles: \"\n",
    "#               + str(len(subject_data))\n",
    "#               + \" - Exploration Session \"\n",
    "#               + str(EXP_session))\n",
    "\n",
    "#         ET_session_count = 0 # session count\n",
    "\n",
    "# # --------- third layer - eye tracking session loop ---------\n",
    "\n",
    "#         # loop over separate eye tracking sessions\n",
    "#         for fileName in subject_data:\n",
    "#             ET_session_count+=1\n",
    "\n",
    "#             print('load data of file ', fileName)\n",
    "\n",
    "#             print('Path: ', DATA_PATH + str(subject) + '/' + fileName)\n",
    "#             # open the JSON file as dictionary\n",
    "#             with open(DATA_PATH + str(subject) + '/' + fileName) as datafile:\n",
    "#                 try:\n",
    "#                     print(\"read file\")\n",
    "#                     dataR = '['+ datafile.read()\n",
    "#                     dataR = dataR[:len(dataR)] + \"]\"\n",
    "#                 except:\n",
    "#                     print(\"reading did not work\")\n",
    "\n",
    "#                 subject_session = json.loads(dataR)\n",
    "#                 print(\" data loaded\")\n",
    "\n",
    "\n",
    "\n",
    "# ##################################################################################################################\n",
    "\n",
    "# #                 # Data flattening part: \n",
    "# #                 # first save trial information\n",
    "\n",
    "# #                 currentTrialInfo = pd.json_normalize(subject_session[0]['trials'][0])\n",
    "# #                 currentTrialInfo = infoDF.drop(columns=['dataPoints'])\n",
    "# #                 currentTrialInfo.insert(0,'Participant',subject)\n",
    "# #                 currentTrialInfo.insert(1,'Session',ET_session)\n",
    "\n",
    "# #                 subjectData_df = pd.concat[subjectData_df, currentTrialInfo]\n",
    "\n",
    "\n",
    "#             # create empty data overview data frame\n",
    "#             flatData_df = pd.DataFrame(columns = allColumnNames)\n",
    "        \n",
    "#             # flatten the majority of the variables into currentDF data frame\n",
    "#             #currentDF_raw = pd.json_normalize(subject_session[0]['trials'][0]['dataPoints'])\n",
    "\n",
    "#             # remove the 'rayCastHitsCombinedEyes' column as it still contains a nested data structure\n",
    "#             #currentDF = currentDF_raw.drop(columns=['rayCastHitsCombinedEyes'])\n",
    "            \n",
    "#             # create an empty data frame of the required size\n",
    "#             #rayCastData_df = pd.DataFrame(np.nan,index=len(subject_session[0]['trials'][0]['dataPoints'], columns= columnsRCall)\n",
    "\n",
    "            \n",
    "#             start = timer()\n",
    "#             # now loop through the individual trials and flatten the data\n",
    "#             for index in range(20):#range(len(subject_session[0]['trials'][0]['dataPoints'])):\n",
    "                \n",
    "#                 # flatten the majority of the variables into currentDF data frame\n",
    "#                 currentDF_raw = pd.json_normalize(subject_session[0]['trials'][0]['dataPoints'][index])\n",
    "\n",
    "#                 # remove the 'rayCastHitsCombinedEyes' column as it still contains a nested data structure\n",
    "#                 currentDF = currentDF_raw.drop(columns=['rayCastHitsCombinedEyes'])\n",
    "            \n",
    "                \n",
    "#                 # depending on the size of the ray cast data - flatten data and appand it to currentDF data frame\n",
    "#                 # the variables are renamed to make the differentiation of first and second order collider hits more intuitive\n",
    "#                 #lengthRCData = len(subject_session[0]['trials'][0]['dataPoints'][index]['rayCastHitsCombinedEyes'][0])\n",
    "#                 lengthRCData = len(currentDF_raw['rayCastHitsCombinedEyes'][0])\n",
    "                \n",
    "#                 if lengthRCData ==0: #case: no ray cast data is available = no collider was hit\n",
    "\n",
    "#                     combineDF = pd.concat([currentDF, emptyDF1, emptyDF2], axis=1)\n",
    "#                     combineDF.insert(len(combineDF.columns), 'dataRow',index)\n",
    "\n",
    "\n",
    "#                 elif lengthRCData == 1: # case: only one collider was hit, there is no secondary hit\n",
    "\n",
    "#                     pdRC1= pd.json_normalize(currentDF_raw['rayCastHitsCombinedEyes'][0][0]).rename(\n",
    "#                         columns = {'hitObjectColliderName':'hitObjectColliderName_1',\n",
    "#                                    'ordinalOfHit':'ordinalOfHit_1',\n",
    "#                                    'hitPointOnObject.x':'hitPointOnObject.x_1',\n",
    "#                                    'hitPointOnObject.y':'hitPointOnObject.y_1',\n",
    "#                                    'hitPointOnObject.z':'hitPointOnObject.z_1',\n",
    "#                                    'hitObjectColliderBoundsCenter.x':'hitObjectColliderBoundsCenter.x_1',\n",
    "#                                    'hitObjectColliderBoundsCenter.y':'hitObjectColliderBoundsCenter.y_1',\n",
    "#                                    'hitObjectColliderBoundsCenter.z':'hitObjectColliderBoundsCenter.z_1'})\n",
    "#                     combineDF = pd.concat([currentDF, pdRC1, emptyDF2], axis=1)\n",
    "#                     combineDF.insert(len(combineDF.columns), 'dataRow',index)\n",
    "\n",
    "#                 elif lengthRCData == 2: # case: two collider were hit \n",
    "\n",
    "#                     pdRC1= pd.json_normalize(currentDF_raw['rayCastHitsCombinedEyes'][0][0]).rename(\n",
    "#                         columns = {'hitObjectColliderName':'hitObjectColliderName_1',\n",
    "#                                    'ordinalOfHit':'ordinalOfHit_1',\n",
    "#                                    'hitPointOnObject.x':'hitPointOnObject.x_1',\n",
    "#                                    'hitPointOnObject.y':'hitPointOnObject.y_1',\n",
    "#                                    'hitPointOnObject.z':'hitPointOnObject.z_1',\n",
    "#                                    'hitObjectColliderBoundsCenter.x':'hitObjectColliderBoundsCenter.x_1',\n",
    "#                                    'hitObjectColliderBoundsCenter.y':'hitObjectColliderBoundsCenter.y_1',\n",
    "#                                    'hitObjectColliderBoundsCenter.z':'hitObjectColliderBoundsCenter.z_1'})\n",
    "\n",
    "#                     pdRC2 = pd.json_normalize(currentDF_raw['rayCastHitsCombinedEyes'][0][1]).rename(\n",
    "#                         columns = {'hitObjectColliderName':'hitObjectColliderName_2',\n",
    "#                                    'ordinalOfHit':'ordinalOfHit_2',\n",
    "#                                    'hitPointOnObject.x':'hitPointOnObject.x_2',\n",
    "#                                    'hitPointOnObject.y':'hitPointOnObject.y_2',\n",
    "#                                    'hitPointOnObject.z':'hitPointOnObject.z_2',\n",
    "#                                    'hitObjectColliderBoundsCenter.x':'hitObjectColliderBoundsCenter.x_2',\n",
    "#                                    'hitObjectColliderBoundsCenter.y':'hitObjectColliderBoundsCenter.y_2',\n",
    "#                                    'hitObjectColliderBoundsCenter.z':'hitObjectColliderBoundsCenter.z_2'})\n",
    "#                     combineDF = pd.concat([currentDF, pdRC1, pdRC2], axis=1)\n",
    "#                     combineDF.insert(len(combineDF.columns), 'dataRow',index)\n",
    "\n",
    "\n",
    "#                 else:\n",
    "#                     print('!!!an exception occured in the ray cast data flattening in trial ', index)\n",
    "\n",
    "#                 # now add the new data row to the data overview\n",
    "#                 # rayCastData_df = [rayCastData_df]\n",
    "\n",
    "            \n",
    "#                 flatData_df = pd.concat([flatData_df,combineDF],ignore_index=True)\n",
    "\n",
    "#             print('saving data')\n",
    "#             flatData_df.to_csv(PROCESSED_DATA_PATH + fileName[0:18] + '_flattened.csv', index = False)\n",
    "#             print('data saved')\n",
    "#             end= timer () \n",
    "#             print('timer:', end-start)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 879.60078,
   "position": {
    "height": "901.823px",
    "left": "1593px",
    "right": "20px",
    "top": "115px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
