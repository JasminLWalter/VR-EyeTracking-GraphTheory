{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "050dead6",
   "metadata": {},
   "source": [
    "# Raw_Data_Preprocessing_HA\n",
    "- written by Jasmin L. Walter (jawalter@uos.de)\n",
    "- reads in nested json files and returns flattened csv files\n",
    "- does not change anything in the data, only extracts all variables from all 9 layers of the nested json files and saves them in data frames / csv files unchanged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcec27c0-9e69-477e-af7d-298549aea326",
   "metadata": {},
   "source": [
    "## Important: this script is optimized to run with the raw json recordings from the HumanA Westbrook builds version!!!\n",
    "For example: it also handles the collidertype variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12781ad9-06ec-46ff-b562-4eaedea9a6a8",
   "metadata": {},
   "source": [
    "Purpose: Flatten nested Unity VR eye-tracking JSON logs into per-file CSVs without altering values.\n",
    "Extracts all variables (incl. up to two ray-cast hits per sample) from the nested structure\n",
    "and writes a flattened data frame per recording, plus a trial info summary.\n",
    "Usage:\n",
    "\n",
    "Set DATA_PATH and PROCESSED_DATA_PATH to your raw JSON folder and output folder.\n",
    "Optionally set subIDs to restrict processing; otherwise participants are inferred from DATA_PATH.\n",
    "Run in Python. Inputs (expected JSON naming and structure):\n",
    "Files named like <ParticipantID>Expl_S<Session>ET<ETSession>_<UnixTS>.json (OnQuit.json files are ignored)\n",
    "JSON structure must contain trials[0]['dataPoints'] with 'rayCastHitsCombinedEyes' Outputs (to PROCESSED_DATA_PATH):\n",
    "<prefix>_infoSummary.csv trial-level metadata (no dataPoints), adds FileInfo\n",
    "<prefix>_flattened.csv flattened dataPoints with columns for first/second ray-cast hits; adds 'DataRow' indexing the original row Notes:\n",
    "Ray-cast hits: rows with 0 hits are filled with NaNs (correct column schema); 1 hit fills “_1”, 2 hits fill “_1” and “_2” columns.\n",
    "Files are discovered and sorted per participant and session; OnQuit.json files are excluded. Dependencies:\n",
    "Python >= 3.9\n",
    "pandas, numpy (json, re, os, time, warnings from the standard library) License: GNU General Public License v3.0 (GPL-3.0) (see LICENSE) \"\"\"\n",
    "\n",
    "\n",
    "Note, not all imported functions might be required \n",
    "\n",
    "Required by the current code:\n",
    "\n",
    "os (paths, listing)\n",
    "json (loading JSON)\n",
    "numpy as np (np.nan, unique)\n",
    "re (extracting numeric tokens from filenames)\n",
    "pandas as pd (json_normalize, DataFrame ops, CSV I/O)\n",
    "time (time.ctime for logging)\n",
    "warnings (suppress FutureWarning)\n",
    "\n",
    "\n",
    "Potentially save to remove (but double check whether script keeps running):\n",
    "\n",
    "importlib, check_package, StopExecution (not invoked)\n",
    "cv2\n",
    "matplotlib.pyplot as plt\n",
    "glob\n",
    "scipy.cluster.vq as clusters\n",
    "pandas.plotting.autocorrelation_plot as AC_plot\n",
    "statsmodels.graphics.tsaplots\n",
    "statsmodels.tsa.stattools.acf\n",
    "mpl_toolkits.mplot3d.Axes3D\n",
    "matplotlib.colors.LinearSegmentedColormap\n",
    "from timeit import default_timer as timer\n",
    "numpy imports/aliases already covered; networkx, skimage, sklearn are commented out (fine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfa87cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General configuration\n",
    "import os\n",
    "\n",
    "# install_missing_packages: bool\n",
    "#     A flag indicating if missing packages should be automatically installed\n",
    "install_missing_packages = True\n",
    "\n",
    "# use_conda: bool\n",
    "#     A flag indicating if conda should be used for software installation.\n",
    "#     If False, pip will be used. The default is to use conda if jupyter\n",
    "#     is run in a conda environment.\n",
    "use_conda = 'CONDA_EXE' in os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baed198d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "def check_package(package, pip_pkg: str = None, conda_pkg: str = None):\n",
    "    \"\"\"Check if a given package is installed. If missing install\n",
    "    it (if global flag `install_missing_packages` is True) either with\n",
    "    pip or with conda (depending on `use_conda`).\n",
    "    \"\"\"\n",
    "    if importlib.util.find_spec(package) is not None:\n",
    "        return  # ok, package is already installed\n",
    "\n",
    "    if not install_missing_packages:\n",
    "        raise RuntimeError(f\"{package} is not installed!\")\n",
    "\n",
    "    if use_conda:\n",
    "        import conda.cli\n",
    "        conda.cli.main('conda', 'install',  '-y', conda_pkg or package)\n",
    "    else:\n",
    "        import subprocess\n",
    "        import sys            \n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', pip_pkg or package])\n",
    "        \n",
    "# This is to exit cells without error tracebacks (cosmetic purpose)\n",
    "class StopExecution(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f10e4b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "#import networkx as nx\n",
    "import glob\n",
    "import scipy.cluster.vq as clusters\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "#from sklearn.preprocessing import normalize\n",
    "from pandas.plotting import autocorrelation_plot as AC_plot \n",
    "from statsmodels.graphics import tsaplots\n",
    "from statsmodels.tsa.stattools import acf\n",
    "#from skimage.filters import gaussian\n",
    "from mpl_toolkits.mplot3d import Axes3D \n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ad0097",
   "metadata": {},
   "source": [
    "# Customize to run scripts - paths, subject ids to run etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2c98379",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'E:/WestbrookProject/Human_A_Data/Experiment1/Exploration_short/'\n",
    "\n",
    "PROCESSED_DATA_PATH = 'E:/WestbrookProject/Human_A_Data/Experiment1/pre-processing/'\n",
    "\n",
    "# Getting the Folder without hidden files in ascending order \n",
    "DATA_FOLDER = sorted([f for f in os.listdir(DATA_PATH) if not f.startswith('.')], key=str.lower)\n",
    "PROCESSED_DATA_FOLDER = sorted([f for f in os.listdir(PROCESSED_DATA_PATH) if not f.startswith('.')], key=str.lower)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53292cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 365  479 1754 2258 2361 2693 3246 3310 3572 3976 4176 4597 4796 4917\n",
      " 5238 5531 5741 6642 7093 7264 7412 7842 8007 8469 8673 8695 9472 9502\n",
      " 9586 9601]\n",
      "number data folders: 30\n"
     ]
    }
   ],
   "source": [
    "subIDs = []\n",
    "for sub in DATA_FOLDER:\n",
    "    if sub[0:4].isdigit():\n",
    "        subIDs.append(int(sub[0:4]))\n",
    "    else:\n",
    "        pass\n",
    "subIDs = np.unique(subIDs)\n",
    "print(subIDs)\n",
    "print('number data folders:', len(subIDs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "24aed9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "# # custom subIDs, if only a subset of participants should be processed\n",
    "subIDs = [1754, 2258, 2361, 2693, 3310, 4176, 4597, 4796, 4917,\n",
    " 5741, 6642, 7093, 7264, 7412, 7842, 8007, 8469, 8673, 9472, 9502,\n",
    " 9586, 9601]\n",
    "#   365,  479, \n",
    "print(len(subIDs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0788e108",
   "metadata": {},
   "source": [
    "# Main part - flatten all nested data structures and save as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "89e8ac41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 1754 started - 1/22 subjects\n",
      "15  files were found for participant  1754\n",
      "A maximum of  5 sessions were found and will be processed\n",
      "A total of  15 files were found and will be processed\n",
      "\tTotal Sessionfiles: 3 - Exploration Session 1\n",
      "load data of file  1754_Expl_S_01_ET_1_1646740718.03586.json\n",
      "Path:  E:/WestbrookProject/Human_A_Data/Experiment1/Exploration_short/1754/1754_Expl_S_01_ET_1_1646740718.03586.json\n",
      "read file\n",
      "data loaded\n",
      "time is:  Wed Jul 31 20:04:33 2024\n",
      "trial info saved\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-18dba35caa48>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m             \u001b[1;31m# flatten the majority of the variables into currentDF data frame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m             \u001b[0mcurrentDF_raw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson_normalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubject_session\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'trials'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dataPoints'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m             \u001b[1;31m# remove the 'rayCastHitsCombinedEyes' column as it still contains a nested data structure\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Software\\Programme\\Anaconda3\\envs\\graphs\\lib\\site-packages\\pandas\\io\\json\\_normalize.py\u001b[0m in \u001b[0;36m_json_normalize\u001b[1;34m(data, record_path, meta, meta_prefix, record_prefix, errors, sep, max_level)\u001b[0m\n\u001b[0;32m    276\u001b[0m             \u001b[1;31m# TODO: handle record value which are lists, at least error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m             \u001b[1;31m#       reasonably\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 278\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnested_to_record\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_level\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_level\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    279\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecord_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Software\\Programme\\Anaconda3\\envs\\graphs\\lib\\site-packages\\pandas\\io\\json\\_normalize.py\u001b[0m in \u001b[0;36mnested_to_record\u001b[1;34m(ds, prefix, sep, level, max_level)\u001b[0m\n\u001b[0;32m     93\u001b[0m             \u001b[1;31m# only dicts gets recurse-flattened\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m             \u001b[1;31m# only at level>1 do we rename the rest of the keys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m             if not isinstance(v, dict) or (\n\u001b[0m\u001b[0;32m     96\u001b[0m                 \u001b[0mmax_level\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlevel\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mmax_level\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m             ):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# if no ray cast information is available, the data frame will be filled with nan values\n",
    "# create empty data frames with nan values and correct variable names\n",
    "columns1 = ['hitObjectColliderName_1','ordinalOfHit_1', 'hitColliderType_1','hitPointOnObject.x_1','hitPointOnObject.y_1','hitPointOnObject.z_1',\n",
    "            'hitObjectColliderBoundsCenter.x_1','hitObjectColliderBoundsCenter.y_1','hitObjectColliderBoundsCenter.z_1']\n",
    "\n",
    "columns2 = ['hitObjectColliderName_2','ordinalOfHit_2','hitColliderType_2','hitPointOnObject.x_2','hitPointOnObject.y_2','hitPointOnObject.z_2',\n",
    "            'hitObjectColliderBoundsCenter.x_2','hitObjectColliderBoundsCenter.y_2','hitObjectColliderBoundsCenter.z_2']\n",
    "\n",
    "columnsRCall = ['hitObjectColliderName_1','ordinalOfHit_1','hitColliderType_1',\n",
    "                'hitPointOnObject.x_1','hitPointOnObject.y_1','hitPointOnObject.z_1',\n",
    "                'hitObjectColliderBoundsCenter.x_1','hitObjectColliderBoundsCenter.y_1','hitObjectColliderBoundsCenter.z_1',\n",
    "                'hitObjectColliderName_2','ordinalOfHit_2','hitColliderType_2',\n",
    "                'hitPointOnObject.x_2','hitPointOnObject.y_2','hitPointOnObject.z_2',\n",
    "                'hitObjectColliderBoundsCenter.x_2','hitObjectColliderBoundsCenter.y_2','hitObjectColliderBoundsCenter.z_2',\n",
    "                'DataRow']\n",
    "\n",
    "emptyDF1 = pd.DataFrame(np.nan,index=[0], columns= columns1)\n",
    "emptyDF2 = pd.DataFrame(np.nan,index=[0], columns= columns2)\n",
    "\n",
    "\n",
    "\n",
    "#########################################################################################################\n",
    "# data loop through all subjects and sessions\n",
    "\n",
    "subcount = 0\n",
    "\n",
    "\n",
    "for subject in subIDs:\n",
    "    \n",
    "    subcount +=1\n",
    "    print('Subject ' \n",
    "          + str(subject) \n",
    "          + ' started - ' \n",
    "          + str(subcount) \n",
    "          + '/' \n",
    "          + str(len(subIDs)) \n",
    "          + ' subjects')\n",
    "    \n",
    "#     # Create empty dataframe for later concatenation\n",
    "# complete_exploration_df = pd.DataFrame(columns = col_names)\n",
    "#     complete_exploration_df.head()\n",
    "    \n",
    "    \n",
    "    # change dir into the subject folder \n",
    "    CURRENT_SUBJECT_FOLDER = sorted([f for f in os.listdir(DATA_PATH+str(subject)) if not f.startswith('.')], key=str.lower)\n",
    "    # get the data files according to the subject, ignoring OnQuit files\n",
    "    subject_files = sorted([f for f in CURRENT_SUBJECT_FOLDER \n",
    "                             if f.startswith(str(subject)+'_Expl_S_') and f.endswith(\"OnQuit.json\") == False], \n",
    "                            key=str.lower) \n",
    "    \n",
    "    # the following works as long as the data name format is as follows:\n",
    "    # 'subjectID'_Expl_S_'SessionNumber'_ET_'EyeTrackingSessionNumber'_'UnixTimestamp'.json\n",
    "    folder_files = list()\n",
    "    \n",
    "    # loop through the subject folder and save all numbers\n",
    "    for file in subject_files:\n",
    "        folder_files.append(re.findall(r'\\d+', file))\n",
    "    \n",
    "    # Extract all SubIDs (only one), SessionNumbers, ET_SessionNumbers (and Timestamps)\n",
    "    try:\n",
    "        SubID, SessionNumbers, ET_SessionNumbers, UnixTimestamp1, UnixTimeStamp2 = map(list, zip(*folder_files))\n",
    "\n",
    "    except:\n",
    "        print('\\tSubject ' \n",
    "              + str(subject)\n",
    "              + ' Filename is not valid!')\n",
    "        \n",
    "#     print(SubID)\n",
    "#     print(SessionNumbers)\n",
    "#     print(ET_SessionNumbers)\n",
    "#     print(UnixTimestamp1)\n",
    "#     print(UnixTimeStamp2)\n",
    "    \n",
    "    session_number = int(max(SessionNumbers)) # the maximum session number of the particular subject\n",
    "    ET_session_number = int(max(ET_SessionNumbers)) # the maximum ET session number of the particular subject\n",
    "    \n",
    "    \n",
    "    # print info of how many files were found \n",
    "    \n",
    "    print(len(SubID), ' files were found for participant ', SubID[0])\n",
    "    print('A maximum of ', session_number, 'sessions were found and will be processed')\n",
    "    print('A total of ', session_number*ET_session_number, 'files were found and will be processed')\n",
    "\n",
    "        \n",
    "# --------- second layer - exploration session loop ---------\n",
    "\n",
    "    # loop over exploration sessions\n",
    "    for EXP_session in range(session_number):\n",
    "        # to avoid start at 0\n",
    "        EXP_session +=1 \n",
    "\n",
    "        # extract the exploration data files for each session - but exclude OnQuit files\n",
    "        subject_data = sorted([f for f in CURRENT_SUBJECT_FOLDER if f.startswith(str(subject) + '_Expl_S_0' + str(EXP_session)) \n",
    "                               and f.endswith(\"OnQuit.json\") == False], key=str.lower)\n",
    "\n",
    "\n",
    "        print(\"\\tTotal Sessionfiles: \"\n",
    "              + str(len(subject_data))\n",
    "              + \" - Exploration Session \"\n",
    "              + str(EXP_session))\n",
    "\n",
    "        ET_session_count = 0 # session count\n",
    "\n",
    "# --------- third layer - eye tracking session loop ---------\n",
    "\n",
    "        # loop over separate eye tracking sessions\n",
    "        for fileName in subject_data:\n",
    "            ET_session_count+=1\n",
    "\n",
    "            print('load data of file ', fileName)\n",
    "\n",
    "            print('Path: ', DATA_PATH + str(subject) + '/' + fileName)\n",
    "            # open the JSON file as dictionary\n",
    "            with open(DATA_PATH + str(subject) + '/' + fileName) as datafile:\n",
    "                try:\n",
    "                    print(\"read file\")\n",
    "                    dataR = '['+ datafile.read()\n",
    "                    dataR = dataR[:len(dataR)] + \"]\"\n",
    "                except:\n",
    "                    print(\"reading did not work\")\n",
    "\n",
    "                subject_session = json.loads(dataR)\n",
    "                print(\"data loaded\")\n",
    "                print('time is: ', time.ctime())\n",
    "\n",
    "\n",
    "\n",
    "##################################################################################################################\n",
    "\n",
    "            # Data flattening part: \n",
    "            # first save the overall trial information\n",
    "\n",
    "\n",
    "            infoDF = pd.json_normalize(subject_session[0]['trials'][0])\n",
    "            infoDF = infoDF.drop(columns=['dataPoints'])\n",
    "            infoDF.insert(0,'FileInfo',fileName[0:18])\n",
    "#             infoDF.to_csv(PROCESSED_DATA_PATH + fileName[0:18] + '_infoSummary.csv', index = False)\n",
    "            infoDF.to_csv(PROCESSED_DATA_PATH + fileName[0:12] + fileName[13:19] + '_infoSummary.csv', index = False)\n",
    "\n",
    "            \n",
    "            print('trial info saved')\n",
    "            \n",
    "\n",
    "            # flatten the majority of the variables into currentDF data frame\n",
    "            currentDF_raw = pd.json_normalize(subject_session[0]['trials'][0]['dataPoints'])\n",
    "\n",
    "            # remove the 'rayCastHitsCombinedEyes' column as it still contains a nested data structure\n",
    "            dataDF = currentDF_raw.drop(columns=['rayCastHitsCombinedEyes'])\n",
    "            \n",
    "            # create an empty data frame of the required size\n",
    "            rayCastData_df = pd.DataFrame(np.nan,index=range(len(subject_session[0]['trials'][0]['dataPoints'])), columns= columnsRCall)\n",
    "\n",
    "            # now loop through the individual trials and flatten the data\n",
    "            for index in range(len(subject_session[0]['trials'][0]['dataPoints'])):\n",
    "                \n",
    "                # depending on the size of the ray cast data - flatten data and appand it to currentDF data frame\n",
    "                # the variables are renamed to make the differentiation of first and second order collider hits more intuitive\n",
    "                #lengthRCData = len(subject_session[0]['trials'][0]['dataPoints'][index]['rayCastHitsCombinedEyes'][0])\n",
    "                lengthRCData = len(currentDF_raw.at[index,'rayCastHitsCombinedEyes'])\n",
    "                \n",
    "                \n",
    "                if lengthRCData ==0: #case: no ray cast data is available = no collider was hit\n",
    "\n",
    "                    combineDF = pd.concat([emptyDF1, emptyDF2], axis=1)\n",
    "                    combineDF.insert(len(combineDF.columns), 'DataRow',index)\n",
    "\n",
    "\n",
    "                elif lengthRCData == 1: # case: only one collider was hit, there is no secondary hit\n",
    "\n",
    "                    pdRC1= pd.json_normalize(currentDF_raw.at[index,'rayCastHitsCombinedEyes'][0]).rename(\n",
    "                        columns = {'hitObjectColliderName':'hitObjectColliderName_1',\n",
    "                                   'ordinalOfHit':'ordinalOfHit_1',\n",
    "                                   'hitColliderType':'hitColliderType_1',\n",
    "                                   'hitPointOnObject.x':'hitPointOnObject.x_1',\n",
    "                                   'hitPointOnObject.y':'hitPointOnObject.y_1',\n",
    "                                   'hitPointOnObject.z':'hitPointOnObject.z_1',\n",
    "                                   'hitObjectColliderBoundsCenter.x':'hitObjectColliderBoundsCenter.x_1',\n",
    "                                   'hitObjectColliderBoundsCenter.y':'hitObjectColliderBoundsCenter.y_1',\n",
    "                                   'hitObjectColliderBoundsCenter.z':'hitObjectColliderBoundsCenter.z_1'})\n",
    "                    combineDF = pd.concat([pdRC1, emptyDF2], axis=1)\n",
    "                    combineDF.insert(len(combineDF.columns), 'DataRow',index)\n",
    "\n",
    "                elif lengthRCData == 2: # case: two collider were hit \n",
    "\n",
    "                    pdRC1= pd.json_normalize(currentDF_raw.at[index,'rayCastHitsCombinedEyes'][0]).rename(\n",
    "                        columns = {'hitObjectColliderName':'hitObjectColliderName_1',\n",
    "                                   'ordinalOfHit':'ordinalOfHit_1',\n",
    "                                   'hitColliderType':'hitColliderType_1',\n",
    "                                   'hitPointOnObject.x':'hitPointOnObject.x_1',\n",
    "                                   'hitPointOnObject.y':'hitPointOnObject.y_1',\n",
    "                                   'hitPointOnObject.z':'hitPointOnObject.z_1',\n",
    "                                   'hitObjectColliderBoundsCenter.x':'hitObjectColliderBoundsCenter.x_1',\n",
    "                                   'hitObjectColliderBoundsCenter.y':'hitObjectColliderBoundsCenter.y_1',\n",
    "                                   'hitObjectColliderBoundsCenter.z':'hitObjectColliderBoundsCenter.z_1'})\n",
    "\n",
    "                    pdRC2 = pd.json_normalize(currentDF_raw.at[index,'rayCastHitsCombinedEyes'][1]).rename(\n",
    "                        columns = {'hitObjectColliderName':'hitObjectColliderName_2',\n",
    "                                   'ordinalOfHit':'ordinalOfHit_2',\n",
    "                                   'hitColliderType':'hitColliderType_2',\n",
    "                                   'hitPointOnObject.x':'hitPointOnObject.x_2',\n",
    "                                   'hitPointOnObject.y':'hitPointOnObject.y_2',\n",
    "                                   'hitPointOnObject.z':'hitPointOnObject.z_2',\n",
    "                                   'hitObjectColliderBoundsCenter.x':'hitObjectColliderBoundsCenter.x_2',\n",
    "                                   'hitObjectColliderBoundsCenter.y':'hitObjectColliderBoundsCenter.y_2',\n",
    "                                   'hitObjectColliderBoundsCenter.z':'hitObjectColliderBoundsCenter.z_2'})\n",
    "                    combineDF = pd.concat([pdRC1, pdRC2], axis=1)\n",
    "                    combineDF.insert(len(combineDF.columns), 'DataRow',index)\n",
    "\n",
    "\n",
    "                else:\n",
    "                    print('!!!an exception occured in the ray cast data flattening in trial ', index)\n",
    "\n",
    "                # now add the new data row to the data overview\n",
    "                # rayCastData_df = [rayCastData_df]\n",
    "\n",
    "            \n",
    "                rayCastData_df.loc[index] = combineDF.loc[0]\n",
    "                \n",
    "            flatData_df = pd.concat([dataDF,rayCastData_df],axis=1)   \n",
    "\n",
    "            print('saving data')\n",
    "#             flatData_df.to_csv(PROCESSED_DATA_PATH + fileName[0:18] + '_flattened.csv', index = False)\n",
    "            flatData_df.to_csv(PROCESSED_DATA_PATH + fileName[0:12] + fileName[13:19] + '_flattened.csv', index = False)\n",
    "\n",
    "            print('data saved')\n",
    "            print('time is: ', time.ctime())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "22b2107c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1754_Expl_S_1_ET_1\n"
     ]
    }
   ],
   "source": [
    "print( fileName[0:12] + fileName[13:19])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f00a069",
   "metadata": {},
   "source": [
    "# ----------------- end of script ------------------------------ (old version below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86a5673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####################################### old Version ##############################################\n",
    "# # if no ray cast information is available, the data frame will be filled with nan values\n",
    "# # create empty data frames with nan values and correct variable names\n",
    "# columns1 = ['hitObjectColliderName_1','ordinalOfHit_1','hitPointOnObject.x_1','hitPointOnObject.y_1','hitPointOnObject.z_1',\n",
    "#             'hitObjectColliderBoundsCenter.x_1','hitObjectColliderBoundsCenter.y_1','hitObjectColliderBoundsCenter.z_1']\n",
    "\n",
    "# columns2 = ['hitObjectColliderName_2','ordinalOfHit_2','hitPointOnObject.x_2','hitPointOnObject.y_2','hitPointOnObject.z_2',\n",
    "#             'hitObjectColliderBoundsCenter.x_2','hitObjectColliderBoundsCenter.y_2','hitObjectColliderBoundsCenter.z_2']\n",
    "\n",
    "# columnsRCall = ['hitObjectColliderName_1','ordinalOfHit_1','hitPointOnObject.x_1','hitPointOnObject.y_1','hitPointOnObject.z_1',\n",
    "#                 'hitObjectColliderBoundsCenter.x_1','hitObjectColliderBoundsCenter.y_1','hitObjectColliderBoundsCenter.z_1',\n",
    "#                 'hitObjectColliderName_2','ordinalOfHit_2','hitPointOnObject.x_2','hitPointOnObject.y_2','hitPointOnObject.z_2',\n",
    "#                 'hitObjectColliderBoundsCenter.x_2','hitObjectColliderBoundsCenter.y_2','hitObjectColliderBoundsCenter.z_2','dataRow']\n",
    "\n",
    "# emptyDF1 = pd.DataFrame(np.nan,index=[0], columns= columns1)\n",
    "# emptyDF2 = pd.DataFrame(np.nan,index=[0], columns= columns2)\n",
    "\n",
    "\n",
    "# # create empty data overview data frame\n",
    "\n",
    "# flatData_df = pd.DataFrame(columns = allColumnNames)\n",
    "\n",
    "# # create empty subject information data frame\n",
    "\n",
    "# subjectData_df = pd.DataFrame(columns = infoVarNames)\n",
    "\n",
    "# #########################################################################################################\n",
    "# # data loop through all subjects and sessions\n",
    "\n",
    "# subcount = 0\n",
    "\n",
    "\n",
    "# for subject in subIDs:\n",
    "    \n",
    "#     subcount +=1\n",
    "#     print('Subject ' \n",
    "#           + str(subject) \n",
    "#           + ' started - ' \n",
    "#           + str(subcount) \n",
    "#           + '/' \n",
    "#           + str(len(subIDs)) \n",
    "#           + ' subjects')\n",
    "    \n",
    "# #     # Create empty dataframe for later concatenation\n",
    "# #     complete_exploration_df = pd.DataFrame(columns = col_names)\n",
    "# #     complete_exploration_df.head()\n",
    "    \n",
    "    \n",
    "#     # change dir into the subject folder \n",
    "#     CURRENT_SUBJECT_FOLDER = sorted([f for f in os.listdir(DATA_PATH+str(subject)) if not f.startswith('.')], key=str.lower)\n",
    "#     # get the data files according to the subject, ignoring OnQuit files\n",
    "#     subject_files = sorted([f for f in CURRENT_SUBJECT_FOLDER \n",
    "#                              if f.startswith(str(subject)+'_Expl_S_') and f.endswith(\"OnQuit.json\") == False], \n",
    "#                             key=str.lower) \n",
    "    \n",
    "#     # the following works as long as the data name format is as follows:\n",
    "#     # 'subjectID'_Expl_S_'SessionNumber'_ET_'EyeTrackingSessionNumber'_'UnixTimestamp'.json\n",
    "#     folder_files = list()\n",
    "    \n",
    "#     # loop through the subject folder and save all numbers\n",
    "#     for file in subject_files:\n",
    "#         folder_files.append(re.findall(r'\\d+', file))\n",
    "    \n",
    "#     # Extract all SubIDs (only one), SessionNumbers, ET_SessionNumbers (and Timestamps)\n",
    "#     try:\n",
    "#         SubID, SessionNumbers, ET_SessionNumbers, UnixTimestamp1, UnixTimeStamp2 = map(list, zip(*folder_files))\n",
    "#     except:\n",
    "#         print('\\tSubject ' \n",
    "#               + str(subject)\n",
    "#               + ' Filename is not valid!')\n",
    "        \n",
    "# #     print(SubID)\n",
    "# #     print(SessionNumbers)\n",
    "# #     print(ET_SessionNumbers)\n",
    "# #     print(UnixTimestamp1)\n",
    "# #     print(UnixTimeStamp2)\n",
    "    \n",
    "#     session_number = int(max(SessionNumbers)) # the maximum session number of the particular subject\n",
    "#     ET_session_number = int(max(ET_SessionNumbers)) # the maximum ET session number of the particular subject\n",
    "    \n",
    "    \n",
    "#     # print info of how many files were found \n",
    "    \n",
    "#     print(len(SubID), ' files were found for participant ', SubID[0])\n",
    "#     print('A maximum of ', session_number, 'sessions were found and will be processed')\n",
    "        \n",
    "# # --------- second layer - exploration session loop ---------\n",
    "\n",
    "#     # loop over exploration sessions\n",
    "#     for EXP_session in range(session_number):\n",
    "#         # to avoid start at 0\n",
    "#         EXP_session +=1 \n",
    "\n",
    "#         # extract the exploration data files for each session - but exclude OnQuit files\n",
    "#         subject_data = sorted([f for f in CURRENT_SUBJECT_FOLDER if f.startswith(str(subject) + '_Expl_S_' + str(EXP_session)) \n",
    "#                                and f.endswith(\"OnQuit.json\") == False], key=str.lower)\n",
    "\n",
    "\n",
    "#         print(\"\\tTotal Sessionfiles: \"\n",
    "#               + str(len(subject_data))\n",
    "#               + \" - Exploration Session \"\n",
    "#               + str(EXP_session))\n",
    "\n",
    "#         ET_session_count = 0 # session count\n",
    "\n",
    "# # --------- third layer - eye tracking session loop ---------\n",
    "\n",
    "#         # loop over separate eye tracking sessions\n",
    "#         for fileName in subject_data:\n",
    "#             ET_session_count+=1\n",
    "\n",
    "#             print('load data of file ', fileName)\n",
    "\n",
    "#             print('Path: ', DATA_PATH + str(subject) + '/' + fileName)\n",
    "#             # open the JSON file as dictionary\n",
    "#             with open(DATA_PATH + str(subject) + '/' + fileName) as datafile:\n",
    "#                 try:\n",
    "#                     print(\"read file\")\n",
    "#                     dataR = '['+ datafile.read()\n",
    "#                     dataR = dataR[:len(dataR)] + \"]\"\n",
    "#                 except:\n",
    "#                     print(\"reading did not work\")\n",
    "\n",
    "#                 subject_session = json.loads(dataR)\n",
    "#                 print(\" data loaded\")\n",
    "\n",
    "\n",
    "\n",
    "# ##################################################################################################################\n",
    "\n",
    "# #                 # Data flattening part: \n",
    "# #                 # first save trial information\n",
    "\n",
    "# #                 currentTrialInfo = pd.json_normalize(subject_session[0]['trials'][0])\n",
    "# #                 currentTrialInfo = infoDF.drop(columns=['dataPoints'])\n",
    "# #                 currentTrialInfo.insert(0,'Participant',subject)\n",
    "# #                 currentTrialInfo.insert(1,'Session',ET_session)\n",
    "\n",
    "# #                 subjectData_df = pd.concat[subjectData_df, currentTrialInfo]\n",
    "\n",
    "\n",
    "#             # create empty data overview data frame\n",
    "#             flatData_df = pd.DataFrame(columns = allColumnNames)\n",
    "        \n",
    "#             # flatten the majority of the variables into currentDF data frame\n",
    "#             #currentDF_raw = pd.json_normalize(subject_session[0]['trials'][0]['dataPoints'])\n",
    "\n",
    "#             # remove the 'rayCastHitsCombinedEyes' column as it still contains a nested data structure\n",
    "#             #currentDF = currentDF_raw.drop(columns=['rayCastHitsCombinedEyes'])\n",
    "            \n",
    "#             # create an empty data frame of the required size\n",
    "#             #rayCastData_df = pd.DataFrame(np.nan,index=len(subject_session[0]['trials'][0]['dataPoints'], columns= columnsRCall)\n",
    "\n",
    "            \n",
    "#             start = timer()\n",
    "#             # now loop through the individual trials and flatten the data\n",
    "#             for index in range(20):#range(len(subject_session[0]['trials'][0]['dataPoints'])):\n",
    "                \n",
    "#                 # flatten the majority of the variables into currentDF data frame\n",
    "#                 currentDF_raw = pd.json_normalize(subject_session[0]['trials'][0]['dataPoints'][index])\n",
    "\n",
    "#                 # remove the 'rayCastHitsCombinedEyes' column as it still contains a nested data structure\n",
    "#                 currentDF = currentDF_raw.drop(columns=['rayCastHitsCombinedEyes'])\n",
    "            \n",
    "                \n",
    "#                 # depending on the size of the ray cast data - flatten data and appand it to currentDF data frame\n",
    "#                 # the variables are renamed to make the differentiation of first and second order collider hits more intuitive\n",
    "#                 #lengthRCData = len(subject_session[0]['trials'][0]['dataPoints'][index]['rayCastHitsCombinedEyes'][0])\n",
    "#                 lengthRCData = len(currentDF_raw['rayCastHitsCombinedEyes'][0])\n",
    "                \n",
    "#                 if lengthRCData ==0: #case: no ray cast data is available = no collider was hit\n",
    "\n",
    "#                     combineDF = pd.concat([currentDF, emptyDF1, emptyDF2], axis=1)\n",
    "#                     combineDF.insert(len(combineDF.columns), 'dataRow',index)\n",
    "\n",
    "\n",
    "#                 elif lengthRCData == 1: # case: only one collider was hit, there is no secondary hit\n",
    "\n",
    "#                     pdRC1= pd.json_normalize(currentDF_raw['rayCastHitsCombinedEyes'][0][0]).rename(\n",
    "#                         columns = {'hitObjectColliderName':'hitObjectColliderName_1',\n",
    "#                                    'ordinalOfHit':'ordinalOfHit_1',\n",
    "#                                    'hitPointOnObject.x':'hitPointOnObject.x_1',\n",
    "#                                    'hitPointOnObject.y':'hitPointOnObject.y_1',\n",
    "#                                    'hitPointOnObject.z':'hitPointOnObject.z_1',\n",
    "#                                    'hitObjectColliderBoundsCenter.x':'hitObjectColliderBoundsCenter.x_1',\n",
    "#                                    'hitObjectColliderBoundsCenter.y':'hitObjectColliderBoundsCenter.y_1',\n",
    "#                                    'hitObjectColliderBoundsCenter.z':'hitObjectColliderBoundsCenter.z_1'})\n",
    "#                     combineDF = pd.concat([currentDF, pdRC1, emptyDF2], axis=1)\n",
    "#                     combineDF.insert(len(combineDF.columns), 'dataRow',index)\n",
    "\n",
    "#                 elif lengthRCData == 2: # case: two collider were hit \n",
    "\n",
    "#                     pdRC1= pd.json_normalize(currentDF_raw['rayCastHitsCombinedEyes'][0][0]).rename(\n",
    "#                         columns = {'hitObjectColliderName':'hitObjectColliderName_1',\n",
    "#                                    'ordinalOfHit':'ordinalOfHit_1',\n",
    "#                                    'hitPointOnObject.x':'hitPointOnObject.x_1',\n",
    "#                                    'hitPointOnObject.y':'hitPointOnObject.y_1',\n",
    "#                                    'hitPointOnObject.z':'hitPointOnObject.z_1',\n",
    "#                                    'hitObjectColliderBoundsCenter.x':'hitObjectColliderBoundsCenter.x_1',\n",
    "#                                    'hitObjectColliderBoundsCenter.y':'hitObjectColliderBoundsCenter.y_1',\n",
    "#                                    'hitObjectColliderBoundsCenter.z':'hitObjectColliderBoundsCenter.z_1'})\n",
    "\n",
    "#                     pdRC2 = pd.json_normalize(currentDF_raw['rayCastHitsCombinedEyes'][0][1]).rename(\n",
    "#                         columns = {'hitObjectColliderName':'hitObjectColliderName_2',\n",
    "#                                    'ordinalOfHit':'ordinalOfHit_2',\n",
    "#                                    'hitPointOnObject.x':'hitPointOnObject.x_2',\n",
    "#                                    'hitPointOnObject.y':'hitPointOnObject.y_2',\n",
    "#                                    'hitPointOnObject.z':'hitPointOnObject.z_2',\n",
    "#                                    'hitObjectColliderBoundsCenter.x':'hitObjectColliderBoundsCenter.x_2',\n",
    "#                                    'hitObjectColliderBoundsCenter.y':'hitObjectColliderBoundsCenter.y_2',\n",
    "#                                    'hitObjectColliderBoundsCenter.z':'hitObjectColliderBoundsCenter.z_2'})\n",
    "#                     combineDF = pd.concat([currentDF, pdRC1, pdRC2], axis=1)\n",
    "#                     combineDF.insert(len(combineDF.columns), 'dataRow',index)\n",
    "\n",
    "\n",
    "#                 else:\n",
    "#                     print('!!!an exception occured in the ray cast data flattening in trial ', index)\n",
    "\n",
    "#                 # now add the new data row to the data overview\n",
    "#                 # rayCastData_df = [rayCastData_df]\n",
    "\n",
    "            \n",
    "#                 flatData_df = pd.concat([flatData_df,combineDF],ignore_index=True)\n",
    "\n",
    "#             print('saving data')\n",
    "#             flatData_df.to_csv(PROCESSED_DATA_PATH + fileName[0:18] + '_flattened.csv', index = False)\n",
    "#             print('data saved')\n",
    "#             end= timer () \n",
    "#             print('timer:', end-start)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 879.60078,
   "position": {
    "height": "901.823px",
    "left": "1593px",
    "right": "20px",
    "top": "115px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
